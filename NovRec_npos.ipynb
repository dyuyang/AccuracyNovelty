{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T07:38:10.310489Z",
     "start_time": "2018-11-27T07:37:52.665978Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import time\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from scipy import spatial\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import scipy\n",
    "from scipy import sparse\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import copy\n",
    "from joblib import *\n",
    "SEED = int(open(\"SEED.txt\", \"r\").readlines()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T07:38:10.342906Z",
     "start_time": "2018-11-27T07:38:10.316473Z"
    }
   },
   "outputs": [],
   "source": [
    "# In[2]:\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--npos', type=str, default=0,help='neg or not pos')\n",
    "parser.add_argument('--beta', type=str, default='',help='beta')\n",
    "parser.add_argument('--iter', type=str, default=0,help='max iteration')\n",
    "parser.add_argument('--dist', type=str, default='',help='distant type')\n",
    "parser.add_argument('--nov', type=str, default='',help='novelty type')\n",
    "parser.add_argument('--baseline', type=str, default=0,help='whether to run baseline')\n",
    "\n",
    "FLAGS, _ = parser.parse_known_args()\n",
    "\n",
    "DATA_DIR='../../data/ml-1m/'\n",
    "MODEL_DIR='./'\n",
    "\n",
    "\n",
    "NPOS_FLG=1#int(FLAGS.npos)\n",
    "DISTANT_TYPE=0#int(FLAGS.dist)\n",
    "NOVELTY_TYPE=1#int(FLAGS.nov)\n",
    "BASELINE=1#int(FLAGS.baseline)\n",
    "\n",
    "assert(DISTANT_TYPE==0 or DISTANT_TYPE==1)\n",
    "assert(NOVELTY_TYPE==0 or NOVELTY_TYPE==1)\n",
    "\n",
    "DATASET_NAME=\"ml\"\n",
    "\n",
    "DATASETOBJ_PATH = '%s_dataset_%d.pkl'%(DATASET_NAME,SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T07:38:10.416547Z",
     "start_time": "2018-11-27T07:38:10.349408Z"
    }
   },
   "outputs": [],
   "source": [
    "class MovieLens:\n",
    "    def load_raw_data(self):\n",
    "        f=tf.gfile.Open(DATA_DIR + 'ratings.dat',\"r\")\n",
    "        self.df_rating = pd.read_csv(\n",
    "            f,\n",
    "            sep='::',\n",
    "            names=['uid', 'itemid', 'rating', 'time'])\n",
    "        \n",
    "        f=tf.gfile.Open(DATA_DIR + 'users.dat',\"r\")\n",
    "        self.df_userinfo = pd.read_csv(\n",
    "            f,\n",
    "            sep='::',\n",
    "            names=['uid', 'sex', 'age', 'occupation', 'zip_code'])\n",
    "        \n",
    "        list_item_attr = [\n",
    "            'itemid', 'title', \"Action\", \"Adventure\", \"Animation\", \"Children's\",\n",
    "            \"Comedy\", \"Crime\", \"Documentary\", \"Drama\", \"Fantasy\", \"Film-Noir\",\n",
    "            \"Horror\", \"Musical\", \"Mystery\", \"Romance\", \"Sci-Fi\", \"Thriller\",\n",
    "            \"War\", \"Western\"\n",
    "        ]\n",
    "        item_data=[]\n",
    "        for line in open(DATA_DIR + 'movies.dat',\"r\").readlines():\n",
    "            tmp=line.strip().split('::')\n",
    "            itemid=tmp[0]\n",
    "            title=tmp[1]\n",
    "            tmp=tmp[2].split('|')\n",
    "            genres=[]\n",
    "            for g in (list_item_attr[2:]):\n",
    "                if g in tmp:\n",
    "                    genres.append(1)\n",
    "                else:\n",
    "                    genres.append(0)\n",
    "            item_data.append([itemid,title]+genres)\n",
    "        self.df_iteminfo=pd.DataFrame(data=item_data,columns=list_item_attr)\n",
    "        self.df_userinfo = self.df_userinfo.fillna(0)\n",
    "        self.df_iteminfo = self.df_iteminfo.fillna(0)\n",
    "        self.df_iteminfo[\"itemid\"]=self.df_iteminfo[\"itemid\"].astype(\"int64\")\n",
    "\n",
    "    def feature_engineering(self):\n",
    "        \n",
    "        le=LabelEncoder()\n",
    "        le.fit(self.df_iteminfo[\"itemid\"])\n",
    "        self.df_iteminfo[\"itemid\"]=le.transform(self.df_iteminfo[\"itemid\"])\n",
    "        self.df_rating[\"itemid\"]=le.transform(self.df_rating[\"itemid\"])\n",
    "\n",
    "        le=LabelEncoder()\n",
    "        le.fit(self.df_userinfo[\"uid\"])\n",
    "        self.df_userinfo[\"uid\"]=le.transform(self.df_userinfo[\"uid\"])\n",
    "        self.df_rating[\"uid\"]=le.transform(self.df_rating[\"uid\"])\n",
    "        self.df_iteminfo.drop([ \"title\"],axis=1,inplace=True)\n",
    "        \n",
    "        ##iteminfo\n",
    "        for index,(df_all,num_feat) in enumerate([(self.df_iteminfo,self.item_numerical_attr),\\\n",
    "                                (self.df_userinfo,self.user_numerical_attr)]):\n",
    "            df_obj = df_all.drop(num_feat,axis=1)\n",
    "            df_numeric=df_all[num_feat]\n",
    "            for c in df_obj:\n",
    "                df_obj[c] = (pd.factorize(df_obj[c])[0])\n",
    "            df_all = pd.concat([df_obj,df_numeric], axis=1)\n",
    "            if index==0:\n",
    "                self.df_iteminfo=df_all\n",
    "            else:\n",
    "                self.df_userinfo=df_all\n",
    "\n",
    "    def __init__(self):\n",
    "        self.rating_threshold = 3\n",
    "        self.load_raw_data()\n",
    "        self.df_iteminfo[\"padding\"]=0\n",
    "        self.user_numerical_attr =  [\"age\"]\n",
    "        self.item_numerical_attr = [\"padding\"]\n",
    "        \n",
    "        self.feature_engineering()\n",
    "\n",
    "\n",
    "\n",
    "# In[5]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T07:38:38.089483Z",
     "start_time": "2018-11-27T07:38:10.423527Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\99701\\AppData\\Local\\Continuum\\anaconda3\\envs\\machine learning\\lib\\site-packages\\ipykernel_launcher.py:7: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  import sys\n",
      "C:\\Users\\99701\\AppData\\Local\\Continuum\\anaconda3\\envs\\machine learning\\lib\\site-packages\\ipykernel_launcher.py:13: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "movielens=MovieLens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T07:39:20.836450Z",
     "start_time": "2018-11-27T07:39:20.764643Z"
    }
   },
   "outputs": [],
   "source": [
    "# In[3]:\n",
    "\n",
    "\n",
    "class DataSetProcesser():\n",
    "    def calculate_data(self):\n",
    "        self.list_uid = self.df_userinfo.uid\n",
    "        self.list_itemid = self.df_iteminfo.itemid\n",
    "\n",
    "        self.all_rateduser_byitemid = [[] for itemid in self.list_itemid]\n",
    "        self.all_posuser_byitemid = [[] for itemid in self.list_itemid]\n",
    "        self.all_rateditem_byuid = [[] for uid in self.list_uid]\n",
    "        self.all_positem_byuid = [[] for uid in self.list_uid]\n",
    "        self.all_neguser_byitemid = [ [] for itemid in self.list_itemid]\n",
    "        self.all_negitem_byuid = [[] for uid in self.list_uid]\n",
    "        \n",
    "        self.all_ratings_byitemid=[[] for itemid in self.list_itemid]\n",
    "        self.all_ratings_byitemid_sparse=[[] for itemid in self.list_itemid]\n",
    "        self.all_israted_byitemid_sparse=[[] for itemid in self.list_itemid]\n",
    "        self.all_ratings_uid_byitemid=[[] for itemid in self.list_itemid]\n",
    "\n",
    "        sz = len(self.df_rating)\n",
    "        \n",
    "        rating_mat=self.df_rating.values\n",
    "\n",
    "        for (index, row) in enumerate(rating_mat):\n",
    "            if index % 10000 == 0:\n",
    "                print('Preprocessing Dataset', index, '/', sz)\n",
    "                \n",
    "            uid,itemid,rating=row[0],row[1],row[2]\n",
    "           \n",
    "            self.all_rateduser_byitemid[itemid].append(uid)\n",
    "            self.all_rateditem_byuid[uid].append(itemid)\n",
    "\n",
    "            self.all_ratings_byitemid[itemid].append(rating)\n",
    "            self.all_ratings_uid_byitemid[itemid].append(uid)\n",
    "            \n",
    "            if rating > self.rating_threshold:\n",
    "                self.all_posuser_byitemid[itemid].append(uid)\n",
    "                self.all_positem_byuid[uid].append(itemid)\n",
    "            else:\n",
    "                self.all_neguser_byitemid[itemid].append(uid)\n",
    "                self.all_negitem_byuid[uid].append(itemid)\n",
    "                \n",
    "        self._USER_SIZE_ONLY_NUM = len(self.user_numerical_attr)\n",
    "        self._USER_SIZE_OF_FIELDS = []\n",
    "        for feat in self.df_userinfo.columns:\n",
    "            if feat not in self.user_numerical_attr:\n",
    "                self._USER_SIZE_OF_FIELDS.append(\n",
    "                    len(np.unique(self.df_userinfo[feat])))\n",
    "        for feat in self.user_numerical_attr:\n",
    "            self._USER_SIZE_OF_FIELDS.append(1)\n",
    "\n",
    "        self._USER_SIZE = len(self._USER_SIZE_OF_FIELDS)\n",
    "        self._USER_SIZE_OF_MASK_FIELDS = self._USER_SIZE_OF_FIELDS[:-self.\n",
    "                                                                   _USER_SIZE_ONLY_NUM]\n",
    "        self._USER_SIZE_BIN = sum(self._USER_SIZE_OF_FIELDS)\n",
    "\n",
    "        self._ITEM_SIZE_ONLY_NUM = len(self.item_numerical_attr)\n",
    "\n",
    "        self._ITEM_SIZE_OF_FIELDS = []\n",
    "        for feat in self.df_iteminfo.columns:\n",
    "            if feat in self.item_numerical_attr:\n",
    "                self._ITEM_SIZE_OF_FIELDS.append(1)\n",
    "            else:\n",
    "                self._ITEM_SIZE_OF_FIELDS.append(\n",
    "                    len(np.unique(self.df_iteminfo[feat])))\n",
    "\n",
    "        self._ITEM_SIZE = len(self._ITEM_SIZE_OF_FIELDS)\n",
    "        self._ITEM_SIZE_OF_MASK_FIELDS = self._ITEM_SIZE_OF_FIELDS[:-self.\n",
    "                                                                   _ITEM_SIZE_ONLY_NUM]\n",
    "        self._ITEM_SIZE_BIN = sum(self._ITEM_SIZE_OF_FIELDS)\n",
    "\n",
    "    def split_history(self, dic,ratio):\n",
    "        seed = self.seed\n",
    "        dic1 = [[] for k in range(len(dic))]\n",
    "        dic2 = [[] for k in range(len(dic))]\n",
    "        for index in range(len(dic)):\n",
    "            lst=dic[index]\n",
    "            lenoflist = len(lst)\n",
    "            if lenoflist != 0:\n",
    "                random.Random(seed).shuffle(lst)\n",
    "                dic1[index] = lst[:int(ratio * lenoflist)]\n",
    "                dic2[index] = lst[int(ratio * lenoflist):]\n",
    "            else:\n",
    "                dic1[index] = []\n",
    "                dic2[index] = []\n",
    "        return dic1, dic2\n",
    "\n",
    "    def merge_history(self, dic1, dic2):\n",
    "        return [ dic1[ky] + dic2[ky] for ky in range(len(dic1))]\n",
    "\n",
    "    def reverse_user_and_item(self, dict_byuid):\n",
    "        result = [[] for itemid in self.list_itemid]\n",
    "        for uid in range(len(dict_byuid)):\n",
    "            for itemid in dict_byuid[uid]:\n",
    "                result[itemid].append(uid)\n",
    "        return result\n",
    "\n",
    "    def split_data(self):\n",
    "        #print(self.all_positem_byuid[1])\n",
    "        self.train_positem_byuid, self.test_positem_byuid = self.split_history(\n",
    "            self.all_positem_byuid,self.ratio)\n",
    "        #print(self.all_positem_byuid[1])\n",
    "\n",
    "        self.train_posuser_byitemid, self.test_posuser_byitemid = self.reverse_user_and_item(\n",
    "            self.train_positem_byuid), self.reverse_user_and_item(\n",
    "                self.test_positem_byuid)\n",
    "\n",
    "        self.train_negitem_byuid, self.test_negitem_byuid = self.split_history(\n",
    "            self.all_negitem_byuid,self.ratio)\n",
    "\n",
    "        self.train_neguser_byitemid, self.test_neguser_byitemid = self.reverse_user_and_item(\n",
    "            self.train_negitem_byuid), self.reverse_user_and_item(\n",
    "                self.test_negitem_byuid)\n",
    "\n",
    "        self.train_rateduser_byitemid = self.merge_history(\n",
    "            self.train_posuser_byitemid, self.train_neguser_byitemid)\n",
    "\n",
    "        self.test_rateduser_byitemid = self.merge_history(\n",
    "            self.test_posuser_byitemid, self.test_neguser_byitemid)\n",
    "\n",
    "        self.train_rateditem_byuid = self.merge_history(self.train_positem_byuid,\n",
    "                                                     self.train_negitem_byuid)\n",
    "\n",
    "        self.test_rateditem_byuid = self.merge_history(self.test_positem_byuid,\n",
    "                                                    self.test_negitem_byuid)\n",
    "        \n",
    "        self.train_ratings_byitemid=[[] for itemid in self.list_itemid]\n",
    "        self.train_ratings_byitemid_sparse=[[] for itemid in self.list_itemid]\n",
    "        self.train_israted_byitemid_sparse=[[] for itemid in self.list_itemid]\n",
    "        self.train_ratings_uid_byitemid=[[] for itemid in self.list_itemid]\n",
    "        sz=len(self.list_itemid)\n",
    "        for itemid in range(len(self.list_itemid)):\n",
    "            if itemid % 50 == 0:\n",
    "                print('dense rating',itemid,'/',sz)\n",
    "            for index in range(len(self.all_ratings_uid_byitemid[itemid])):\n",
    "                uid,rating=self.all_ratings_uid_byitemid[itemid][index],self.all_ratings_byitemid[itemid][index]\n",
    "                if uid in self.train_rateduser_byitemid[itemid]:\n",
    "                    self.train_ratings_byitemid[itemid].append(rating)\n",
    "                    self.train_ratings_uid_byitemid[itemid].append(uid)\n",
    "                \n",
    "        usz=len(self.list_uid)\n",
    "        isz=len(self.list_itemid)\n",
    "        for itemid in range(len(self.list_itemid)):\n",
    "            if itemid % 50 == 0:\n",
    "                print('sparse all rating',itemid,'/',isz)     \n",
    "            vec=self.all_ratings_byitemid[itemid]\n",
    "            vec=vec/np.linalg.norm(vec)\n",
    "            self.all_ratings_byitemid_sparse[itemid]=sparse.coo_matrix((vec,\n",
    "                                                                       ([0.0 for k in range(len(vec))],\n",
    "                                                                       self.all_ratings_uid_byitemid[itemid])),shape=(1,usz))\n",
    "            self.all_israted_byitemid_sparse[itemid]=sparse.coo_matrix(([1.0 for k in range(len(vec))],\n",
    "                                                                       ([0.0 for k in range(len(vec))],\n",
    "                                                                       self.all_ratings_uid_byitemid[itemid])),shape=(1,usz))\n",
    "            \n",
    "            vec=self.train_ratings_byitemid[itemid]\n",
    "            vec=vec/np.linalg.norm(vec)\n",
    "            self.train_ratings_byitemid_sparse[itemid]=sparse.coo_matrix((vec,\n",
    "                                                                       ([0.0 for k in range(len(vec))],\n",
    "                                                                       self.train_ratings_uid_byitemid[itemid])),shape=(1,usz))\n",
    "            self.train_israted_byitemid_sparse[itemid]=sparse.coo_matrix(([1.0 for k in range(len(vec))],\n",
    "                                                                       ([0.0 for k in range(len(vec))],\n",
    "                                                                       self.train_ratings_uid_byitemid[itemid])),shape=(1,usz))\n",
    "            \n",
    "            \n",
    "        \n",
    "\n",
    "    def __init__(self, movielens, split_ratio, seed=SEED):\n",
    "        self.seed = seed\n",
    "        self.rating_threshold = movielens.rating_threshold\n",
    "        self.ratio = split_ratio\n",
    "        self.df_rating = movielens.df_rating\n",
    "        self.df_userinfo = movielens.df_userinfo\n",
    "        self.df_iteminfo = movielens.df_iteminfo\n",
    "        self.user_numerical_attr = movielens.user_numerical_attr\n",
    "        self.item_numerical_attr = movielens.item_numerical_attr\n",
    "        self.calculate_data()\n",
    "        self.split_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T07:40:19.682327Z",
     "start_time": "2018-11-27T07:40:11.831292Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    f=open(DATASETOBJ_PATH,\"rb\")\n",
    "    dataset=pickle.load(f)\n",
    "    f.close()\n",
    "except:\n",
    "    movielens=MovieLens()\n",
    "    dataset=DataSetProcesser(movielens,0.7)\n",
    "    f=open(DATASETOBJ_PATH,\"wb\")\n",
    "    pickle.dump(dataset,f)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "# In[6]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T07:40:29.081038Z",
     "start_time": "2018-11-27T07:40:28.197896Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\99701\\AppData\\Local\\Continuum\\anaconda3\\envs\\machine learning\\lib\\site-packages\\ipykernel_launcher.py:29: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\99701\\AppData\\Local\\Continuum\\anaconda3\\envs\\machine learning\\lib\\site-packages\\ipykernel_launcher.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "C:\\Users\\99701\\AppData\\Local\\Continuum\\anaconda3\\envs\\machine learning\\lib\\site-packages\\ipykernel_launcher.py:37: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def check():\n",
    "    rating=movielens.df_rating\n",
    "    user=movielens.df_userinfo\n",
    "    item=movielens.df_iteminfo\n",
    "\n",
    "    uid=10\n",
    "    itemid=12\n",
    "\n",
    "    tmp1=[0 for k in range(len(dataset.list_uid))]\n",
    "    for row in rating[rating.itemid==itemid].values:\n",
    "        tmp1[row[0]]=row[2]\n",
    "    tmp1=np.array(tmp1)/np.linalg.norm(tmp1)\n",
    "\n",
    "    tmp2=dataset.all_ratings_byitemid_sparse[itemid].toarray()[0]\n",
    "\n",
    "    print(sum(tmp1)==sum(tmp2))\n",
    "\n",
    "    tmp1=[0 for k in range(len(dataset.list_uid))]\n",
    "    for row in rating[rating.itemid==itemid].values:\n",
    "        if row[0] in dataset.train_rateduser_byitemid[row[1]]:\n",
    "            tmp1[row[0]]=row[2]\n",
    "    tmp1=np.array(tmp1)/np.linalg.norm(tmp1)\n",
    "\n",
    "    tmp2=dataset.train_ratings_byitemid_sparse[itemid].toarray()[0]\n",
    "\n",
    "    sum(tmp1==tmp2)==len(tmp1)\n",
    "\n",
    "    tmp1=sorted(dataset.all_positem_byuid[uid])\n",
    "    tmp2=sorted(rating[rating.rating>3][rating.uid==uid].itemid.tolist())\n",
    "    print(tmp1==tmp2)\n",
    "\n",
    "    tmp1=sorted(dataset.all_posuser_byitemid[itemid])\n",
    "    tmp2=sorted(rating[rating.rating>3][rating.itemid==itemid].uid.tolist())\n",
    "    print(tmp1==tmp2)\n",
    "    \n",
    "    tmp1=sorted(dataset.all_negitem_byuid[uid])\n",
    "    tmp2=sorted(rating[rating.rating<=3][rating.uid==uid].itemid.tolist())\n",
    "    print(tmp1==tmp2)\n",
    "\n",
    "\n",
    "    tmp1=sorted(dataset.all_rateditem_byuid[uid])\n",
    "    tmp2=sorted(rating[rating.rating!=0][rating.uid==uid].itemid.tolist())\n",
    "    print(tmp1==tmp2)\n",
    "\n",
    "    tmp1=sorted(dataset.all_rateduser_byitemid[itemid])\n",
    "    tmp2=sorted(rating[rating.rating!=0][rating.itemid==itemid].uid.tolist())\n",
    "    print(tmp1==tmp2)\n",
    "\n",
    "    print(set(dataset.train_positem_byuid[uid])|set(dataset.test_positem_byuid[uid])==set(dataset.all_positem_byuid[uid]))\n",
    "    print(set(dataset.train_rateditem_byuid[uid])|set(dataset.test_rateditem_byuid[uid])==set(dataset.all_rateditem_byuid[uid]))\n",
    "    print(set(dataset.train_negitem_byuid[uid])|set(dataset.test_negitem_byuid[uid])==set(dataset.all_negitem_byuid[uid]))\n",
    "    print(set(dataset.all_positem_byuid[uid])|set(dataset.all_negitem_byuid[uid])==set(dataset.all_rateditem_byuid[uid]))\n",
    "    print(set(dataset.train_positem_byuid[uid])|set(dataset.train_negitem_byuid[uid])==set(dataset.train_rateditem_byuid[uid]))\n",
    "    #print(dataset.train_rateditem_byuid[1])\n",
    "check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T07:40:29.770076Z",
     "start_time": "2018-11-27T07:40:29.723202Z"
    }
   },
   "outputs": [],
   "source": [
    "class RecommendSysUtil():\n",
    "\n",
    "    def distant(self, lst1,lst2,is_test=0,dis_type=DISTANT_TYPE):\n",
    "        if is_test==0:\n",
    "            mat1=sparse.vstack([self.dataset.train_ratings_byitemid_sparse[i] if dis_type==1 \\\n",
    "                                   else self.dataset.train_israted_byitemid_sparse[i]\\\n",
    "                                   for i in lst1])\n",
    "        else:\n",
    "            mat1=sparse.vstack([self.dataset.all_ratings_byitemid_sparse[i] if dis_type==1 \\\n",
    "                                   else self.dataset.all_israted_byitemid_sparse[i]\\\n",
    "                                   for i in lst1])\n",
    "        if is_test==0:\n",
    "            mat2=sparse.vstack([self.dataset.train_ratings_byitemid_sparse[i] if dis_type==1 \\\n",
    "                                   else self.dataset.train_israted_byitemid_sparse[i]\\\n",
    "                                   for i in lst2])\n",
    "        else:\n",
    "            mat2=sparse.vstack([self.dataset.all_ratings_byitemid_sparse[i] if dis_type==1 \\\n",
    "                                   else self.dataset.all_israted_byitemid_sparse[i]\\\n",
    "                                   for i in lst2])\n",
    "        if dis_type==1:\n",
    "            dis_mat=1-mat1.dot(mat2.transpose()).toarray()\n",
    "        else:\n",
    "            dis_mat=mat1.dot(mat2.transpose()).toarray()\n",
    "            #print(dis_mat)\n",
    "            for i in range(len(lst1)):\n",
    "                for j in range(len(lst2)):\n",
    "                    sz=len(self.dataset.train_rateduser_byitemid[lst2[j]]) if is_test==0 \\\n",
    "                    else len(self.dataset.all_rateduser_byitemid[lst2[j]])\n",
    "                    #print(dis_mat[index],sz,1.0-1.0*dis_mat[index]/sz)\n",
    "                    dis_mat[i][j]=1.0-1.0*dis_mat[i][j]/sz\n",
    "                #print(dis_mat[index])\n",
    "        return dis_mat\n",
    "    \n",
    "\n",
    "    def novelty(self,n_jobs=1,is_test=0,novelty_type=NOVELTY_TYPE):  \n",
    "        if novelty_type==1:\n",
    "            def novelty_dist(self,uid,is_test=0):\n",
    "                if (is_test==0 and len(self.dataset.train_positem_byuid[uid])==0) or (\n",
    "                    is_test==1 and len(self.dataset.all_positem_byuid[uid])==0):\n",
    "                    return []\n",
    "                #start = time.clock()\n",
    "                if is_test==0:\n",
    "                    pos_lst=list(self.dataset.train_positem_byuid[uid])\n",
    "                else:\n",
    "                    pos_lst=list(self.dataset.all_positem_byuid[uid])\n",
    "                if is_test==0:\n",
    "                    rated_lst=list(self.dataset.train_rateditem_byuid[uid])\n",
    "                else:\n",
    "                    rated_lst=list(self.dataset.all_rateditem_byuid[uid])\n",
    "                dis_mat=self.distant(pos_lst,rated_lst,is_test)\n",
    "                nov_lst=[]\n",
    "                for index_i,i in enumerate(pos_lst):\n",
    "                    nov_lst.append(np.mean([dis_mat[index_i][index_j] for index_j,j in enumerate(rated_lst)]))\n",
    "                return nov_lst\n",
    "            results = Parallel(n_jobs=n_jobs,verbose=100,pre_dispatch='all',batch_size=int(len(self.dataset.list_uid)/n_jobs))(\n",
    "            delayed(novelty_dist)(self,uid,is_test) for uid in dataset.list_uid)\n",
    "        else:\n",
    "            def novelty_popular(self,uid,is_test=0):\n",
    "                if is_test==0:\n",
    "                    nov_lst=[\n",
    "                        -np.log2(1.0*len(self.dataset.train_rateduser_byitemid[itemid]) \\\n",
    "                                 / len(self.dataset.list_uid) + pow(10, -9)) \\\n",
    "                             for itemid in self.dataset.train_positem_byuid[uid]\n",
    "                    ]\n",
    "                else:\n",
    "                    nov_lst=[\n",
    "                        -np.log2(1.0*len(self.dataset.all_rateduser_byitemid[itemid]) \\\n",
    "                                 / len(self.dataset.list_uid) + pow(10, -9)) \\\n",
    "                             for itemid in self.dataset.all_positem_byuid[uid]\n",
    "                    ]\n",
    "                #if uid==0:\n",
    "                    #print(uid,nov_lst)\n",
    "                return nov_lst\n",
    "            results = Parallel(n_jobs=n_jobs,verbose=100,pre_dispatch='all',batch_size=int(len(self.dataset.list_uid)/n_jobs))(\n",
    "            delayed(novelty_popular)(self,uid,is_test) for uid in dataset.list_uid)\n",
    "        return results  \n",
    "    \n",
    "    def item_vectorize(self, itemid):\n",
    "        return [ x for x in self.dataset.df_iteminfo.iloc[itemid,:].values] \n",
    "\n",
    "    def user_vectorize(self, uid):\n",
    "        return [ x for x in self.dataset.df_userinfo.iloc[uid,:].values]  \n",
    "                \n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T07:40:30.975489Z",
     "start_time": "2018-11-27T07:40:30.970501Z"
    }
   },
   "outputs": [],
   "source": [
    "util=RecommendSysUtil(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T07:40:32.004287Z",
     "start_time": "2018-11-27T07:40:31.913181Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6799163179916319\n",
      "0.805732484076433\n",
      "[[0.80573248 0.        ]\n",
      " [0.         0.86593407]]\n",
      "[[0.67991632 0.        ]\n",
      " [0.         0.78174037]]\n",
      "[[-6.66133815e-16  8.66107392e-01  9.20543707e-01]]\n",
      "[[-6.43929354e-15  7.59053552e-01  8.44543398e-01]]\n",
      "0.866107391548371 0.9205437065370933 0.7590535515158896 0.8445433982798506\n"
     ]
    }
   ],
   "source": [
    "def check_dist():\n",
    "    print(1-len(set(dataset.all_rateduser_byitemid[1])&set(dataset.all_rateduser_byitemid[2]))/len(set(dataset.all_rateduser_byitemid[2])))\n",
    "    print(1-len(set(dataset.train_rateduser_byitemid[1])&set(dataset.train_rateduser_byitemid[2]))/len(set(dataset.train_rateduser_byitemid[2])))\n",
    "    \n",
    "    print(util.distant([1,2],[2,1],dis_type=0))\n",
    "\n",
    "    print(util.distant([1,2],[2,1],dis_type=0,is_test=1))\n",
    "\n",
    "    print(util.distant([1],[1,2,3],dis_type=1,is_test=0))\n",
    "\n",
    "    print(util.distant([1],[1,2,3],dis_type=1,is_test=1))\n",
    "    def check(item1,item2,is_test=0):\n",
    "        vec1=[0 for k in range(len(dataset.list_uid))]\n",
    "        vec2=[0 for k in range(len(dataset.list_uid))]\n",
    "\n",
    "        if is_test==0:\n",
    "            for index,uid in enumerate(dataset.train_ratings_uid_byitemid[item1]):\n",
    "                vec1[uid]=dataset.train_ratings_byitemid[item1][index]\n",
    "\n",
    "            for index,uid in enumerate(dataset.train_ratings_uid_byitemid[item2]):\n",
    "                vec2[uid]=dataset.train_ratings_byitemid[item2][index]\n",
    "        else:\n",
    "            for index,uid in enumerate(dataset.all_ratings_uid_byitemid[item1]):\n",
    "                vec1[uid]=dataset.all_ratings_byitemid[item1][index]\n",
    "               # print(vec1[uid])\n",
    "\n",
    "            for index,uid in enumerate(dataset.all_ratings_uid_byitemid[item2]):\n",
    "                vec2[uid]=dataset.all_ratings_byitemid[item2][index]\n",
    "\n",
    "        vec1=np.array(vec1)/np.linalg.norm(vec1)\n",
    "        vec2=np.array(vec2)/np.linalg.norm(vec2)\n",
    "        return 1-vec1.dot(vec2)\n",
    "#     return vec1,vec2\n",
    "    print(check(1,2,is_test=0),check(1,3,is_test=0),check(1,2,is_test=1),check(1,3,is_test=1))\n",
    "check_dist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T08:41:37.333703Z",
     "start_time": "2018-11-27T08:41:37.329713Z"
    }
   },
   "outputs": [],
   "source": [
    "# def check_novelty():\n",
    "#     mat=util.novelty(is_test=0,novelty_type=0)\n",
    "\n",
    "#     -np.log2(1.0*len(self.dataset.train_rateduser_byitemid[itemid]) \\\n",
    "#                                      / len(self.dataset.list_uid) + pow(10, -9)) \\\n",
    "#                                  for itemid in self.dataset.train_positem_byuid[uid]\n",
    "\n",
    "#     -np.log2(1.0*len(dataset.train_rateduser_byitemid[2722]) \\\n",
    "#                                      / len(dataset.list_uid) + pow(10, -9))\n",
    "\n",
    "#     dataset.train_positem_byuid[0]\n",
    "\n",
    "#     mat[0]\n",
    "\n",
    "#     mat2=util.novelty(is_test=1,novelty_type=0)\n",
    "\n",
    "#     mat2[0]\n",
    "\n",
    "#     dataset.all_positem_byuid[0]\n",
    "\n",
    "#      -np.log2(1.0*len(dataset.all_rateduser_byitemid[2722]) \\\n",
    "#                                      / len(dataset.list_uid) + pow(10, -9))\n",
    "\n",
    "#     mat3=util.novelty(is_test=0,novelty_type=1)\n",
    "\n",
    "#     mat4=util.novelty(is_test=1,novelty_type=1)\n",
    "\n",
    "#     mat3[0]\n",
    "\n",
    "#     rated_lst=dataset.train_positem_byuid[0]\n",
    "#     dis_lst=[]\n",
    "#     for item in rated_lst:\n",
    "#         dis_lst.append(util.distant([2722],[item],is_test=0,dis_type=0)[0][0])\n",
    "#     np.mean(dis_lst)\n",
    "\n",
    "#     mat4[0]\n",
    "\n",
    "#     rated_lst=dataset.all_positem_byuid[0]\n",
    "#     dis_lst=[]\n",
    "#     for item in rated_lst:\n",
    "#         dis_lst.append(util.distant([2722],[item],is_test=1,dis_type=0)[0][0])\n",
    "#     np.mean(dis_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T08:42:32.122905Z",
     "start_time": "2018-11-27T08:42:32.069021Z"
    }
   },
   "outputs": [],
   "source": [
    "class RecommendSys():\n",
    "\n",
    "    def get_novelty_distribution(self):\n",
    "        print('Calculating novelty distribution...')\n",
    "        try:\n",
    "            f=open(self.nov_distri_path,\"rb\")\n",
    "            train_distri,all_distri=pickle.load(f)\n",
    "        except:\n",
    "            train_distri=self.util.novelty(n_jobs=1,is_test=0)\n",
    "            all_distri=self.util.novelty(n_jobs=1,is_test=1)\n",
    "            f=open(self.nov_distri_path,\"wb\")\n",
    "            pickle.dump((train_distri,all_distri),f)\n",
    "        \n",
    "        train_distri_pow=[ [pow(nov,self.beta) for nov in lst ] for lst in train_distri ]\n",
    "        train_distri_pow=[ (np.array(lst)/sum(lst)).tolist() for lst in train_distri_pow ]\n",
    "\n",
    "        return train_distri_pow,all_distri\n",
    "\n",
    "    def predict(self, list_uid, list_itemid):\n",
    "        user_batch = [self.util.user_vectorize(uid) for uid in list_uid]\n",
    "        item_batch = [self.util.item_vectorize(itemid) for itemid in list_itemid]\n",
    "        label_batch = [[1] * len(list_itemid) for uid in list_uid]\n",
    "        prob_matrix = self.prob.eval(\n",
    "            feed_dict={\n",
    "                self.user_input: user_batch,\n",
    "                self.item_input: item_batch,\n",
    "                self.label: label_batch\n",
    "            })\n",
    "        return prob_matrix\n",
    "\n",
    "    def predict_by_queue(self, list_uid, list_itemid,n_jobs=1):\n",
    "        sz = len(list_uid)\n",
    "        batch_sz = 5000\n",
    "        bins = int(sz / batch_sz)\n",
    "        def predict_batch(self,idx,list_uid,list_itemid):\n",
    "            return  self.predict(list_uid, list_itemid)\n",
    "        results = Parallel(n_jobs=n_jobs,verbose=100,pre_dispatch='all',batch_size=int(len(self.dataset.list_uid)/n_jobs))(\n",
    "            delayed(predict_batch)(self,idx,list_uid[idx * batch_sz:(idx + 1) * batch_sz],list_itemid) for idx in range(bins))\n",
    "        \n",
    "        results=np.concatenate(results,axis=0)\n",
    "        tmp = self.predict(list_uid[bins * batch_sz:], list_itemid)\n",
    "        #print(np.shape(results),np.shape(tmp))\n",
    "        if results != []:\n",
    "            results = np.concatenate((results, tmp), axis=0)\n",
    "        else:\n",
    "            results = tmp\n",
    "        return results\n",
    "\n",
    "    def eval_performance(self):\n",
    "\n",
    "        list_uid = dataset.list_uid\n",
    "        list_itemid = dataset.list_itemid\n",
    "        self.prob_by_uitem = self.predict_by_queue(list_uid, list_itemid)\n",
    "        self.uid_to_recomm = self.base_recommend(self.prob_by_uitem,\n",
    "                                                 self.top_N)\n",
    "        ndcg,reward0, reward1, agg_div, entro_div = self.measure(\n",
    "            self.uid_to_recomm)\n",
    "        return ndcg, reward0, reward1, agg_div, entro_div\n",
    "\n",
    "    def base_recommend(self, prob_by_uitem,top_N,n_jobs=1):\n",
    "        #print(time.clock())\n",
    "        self.base_uid_to_reomm={}\n",
    "        def recommend_single_user(uid,rated, prob_lst,list_item,top_N):\n",
    "            #start=time.clock()\n",
    "            prob_arr = list(zip(list_item, prob_lst))\n",
    "            prob_arr = sorted(prob_arr, key=lambda d: -d[1])\n",
    "            cnt = 0\n",
    "            recomm= []\n",
    "            for pair in prob_arr:\n",
    "                itemid = pair[0]\n",
    "                rel=pair[1]\n",
    "                if itemid not in rated:\n",
    "                    recomm.append(itemid)\n",
    "                    cnt += 1\n",
    "                    if cnt == top_N:\n",
    "                        break\n",
    "            #print(time.clock()-start)\n",
    "            return (uid,recomm)\n",
    "        uid_to_recomm = Parallel(n_jobs=n_jobs,verbose=100,pre_dispatch='all',batch_size=int(len(self.dataset.list_uid)/n_jobs))(\n",
    "            delayed(recommend_single_user)(uid,dataset.train_rateditem_byuid[uid],prob_by_uitem[uid],self.dataset.list_itemid,top_N) \n",
    "            for uid in list(filter(\n",
    "                lambda uid: len(self.dataset.test_positem_byuid[uid]) >= self.top_N,self.dataset.list_uid)))\n",
    "        self.base_uid_to_recomm=dict(uid_to_recomm)\n",
    "        return self.base_uid_to_recomm\n",
    "\n",
    "    def measure(self, uid_to_recomm):\n",
    "        \n",
    "        ndcg=0.0\n",
    "        #print(\"NDCG: %.4f\"%(ndcg))\n",
    "\n",
    "\n",
    "        #### novelty metric\n",
    "        avg_reward0 = 0.0\n",
    "        avg_reward1 = 0.0\n",
    "        agg_div = 0.0\n",
    "        enp_div = 0.0\n",
    "\n",
    "        cnt = 0\n",
    "        for uid in uid_to_recomm:\n",
    "            reward0 = 0.0\n",
    "            reward1 = 0.0\n",
    "            sz=len(self.dataset.train_positem_byuid[uid])\n",
    "            for itemid in uid_to_recomm[uid]:\n",
    "                if (itemid in self.dataset.test_positem_byuid[uid]):\n",
    "                    nov = self.all_distri[uid][sz+self.dataset.test_positem_byuid[uid].index(itemid)]\n",
    "                    #print(nov)\n",
    "                    nov0 = pow(nov, 0)\n",
    "                    nov1 = pow(nov, 1)\n",
    "                    reward0 = max(reward0, nov0)\n",
    "                    reward1 = max(reward1, nov1)\n",
    "            avg_reward0 += reward0\n",
    "            if reward1!=np.inf and reward1!=-np.inf and reward1 != 0.0 :\n",
    "                avg_reward1 += reward1\n",
    "                #print(reward1)\n",
    "                cnt+=1\n",
    "                \n",
    "\n",
    "        if avg_reward0 != 0:\n",
    "            avg_reward0 /= len(uid_to_recomm)\n",
    "        if avg_reward1 != 0:\n",
    "            avg_reward1 /= cnt\n",
    "        print(\n",
    "            'Novelty: reward(β=0)=%.5f reward(β=1)=%.5f'\n",
    "            % (avg_reward0, avg_reward1))\n",
    "\n",
    "\n",
    "        #### diversity metric\n",
    "        recomm_set = set()\n",
    "        for uid in uid_to_recomm:\n",
    "            recomm_set = recomm_set | set(uid_to_recomm[uid])\n",
    "        agg_div = 1.0*len(recomm_set) / len(uid_to_recomm) / self.top_N\n",
    "\n",
    "        itemid_to_recomuser = {}\n",
    "\n",
    "        for uid in uid_to_recomm:\n",
    "            for itemid in uid_to_recomm[uid]:\n",
    "                if itemid not in itemid_to_recomuser:\n",
    "                    itemid_to_recomuser[itemid] = 0\n",
    "                itemid_to_recomuser[itemid] += 1\n",
    "\n",
    "        s = 0\n",
    "        for itemid in itemid_to_recomuser:\n",
    "            s += itemid_to_recomuser[itemid]\n",
    "\n",
    "        for itemid in itemid_to_recomuser:\n",
    "            probb = 1.0*itemid_to_recomuser[itemid] / s + pow(10, -9)\n",
    "            enp_div += -(np.log2(probb) * probb)\n",
    "\n",
    "        print(\n",
    "            'Diversity: aggdiv=%.5f entropydiv=%.5f'\n",
    "            % (agg_div, enp_div))\n",
    "        return ndcg, avg_reward0, avg_reward1, agg_div, enp_div\n",
    "\n",
    "    # def print_recommend(uid):\n",
    "        \n",
    "    def train_a_batch(self, iter, session):\n",
    "        \n",
    "        loss_all = 0\n",
    "\n",
    "        user_batch = []\n",
    "        item_batch = []\n",
    "        label_batch = []\n",
    "        list_positemid = []\n",
    "        list_uid = []\n",
    "        list_label = []\n",
    "        list_negitemid = []\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            uid = 0\n",
    "            while (True):\n",
    "                uid = self.rng.randint(1, self.NUM_USERS)\n",
    "                dataset = self.dataset\n",
    "                if ((uid in dataset.list_uid)\n",
    "                        and len(dataset.train_positem_byuid[uid]) != 0)#and len(dataset.train_negitem_byuid[uid]) != 0):\n",
    "                    break\n",
    "            list_uid.append(uid)\n",
    "\n",
    "        for uid in list_uid:\n",
    "            pos_itemid = self.rng.choice(\n",
    "                self.dataset.train_positem_byuid[uid], p=self.pos_distri[uid])\n",
    "            list_positemid.append(pos_itemid)\n",
    "            list_label.append(1)\n",
    "            user_batch.append(self.util.user_vectorize(uid))\n",
    "            pos_itemvec = self.util.item_vectorize(pos_itemid)\n",
    "            item_batch.append(pos_itemvec)\n",
    "\n",
    "        prob_by_uitem = self.predict(list_uid, list_positemid)\n",
    "\n",
    "        neg_itemset = set()\n",
    "        neg_index = {}\n",
    "        for uid in list_uid:\n",
    "            if NPOS_FLG==1:\n",
    "                neg_itemset = neg_itemset | (\n",
    "                    set(self.dataset.list_itemid)\n",
    "                    -set(self.dataset.train_positem_byuid[uid])\n",
    "                )\n",
    "            else:\n",
    "                neg_itemset = neg_itemset | set(dataset.train_negitem_byuid[uid])\n",
    "        \n",
    "        neg_itemset=list(neg_itemset)\n",
    "        for index, neg_item in enumerate(neg_itemset):\n",
    "            neg_index[neg_item] = index\n",
    "        neg_prob_by_uitem = self.predict(list_uid, neg_itemset)\n",
    "\n",
    "        violator_cnt = 0\n",
    "        for i, uid in enumerate(list_uid):\n",
    "            neg_itemid = -1\n",
    "            pos_itemid = list_positemid[i]\n",
    "            pos_prob = prob_by_uitem[i][i]\n",
    "            for k in range(self.LIMIT):\n",
    "                \n",
    "                if NPOS_FLG==1:\n",
    "                    neg_item_list=list(\n",
    "                    set(self.dataset.list_itemid)\n",
    "                    -set(self.dataset.train_positem_byuid[uid])\n",
    "                    )\n",
    "                    neg_itemid = self.rng.choice(\n",
    "                        neg_item_list)\n",
    "                else:\n",
    "                    neg_itemid = self.rng.choice(\n",
    "                        self.dataset.train_negitem_byuid[uid])\n",
    "                neg_prob = neg_prob_by_uitem[i][neg_index[neg_itemid]]\n",
    "                if neg_prob >= pos_prob and neg_prob != 0:\n",
    "                    break\n",
    "                else:\n",
    "                    neg_itemid = -1\n",
    "\n",
    "            if neg_itemid != -1:\n",
    "                violator_cnt += 1\n",
    "                list_label.append(-1)\n",
    "                user_batch.append(self.util.user_vectorize(uid))\n",
    "                neg_itemvec = self.util.item_vectorize(neg_itemid)\n",
    "                item_batch.append(neg_itemvec)\n",
    "\n",
    "        label_batch = [[1] * len(user_batch) for j in range(len(user_batch))]\n",
    "        for i, label in enumerate(list_label):\n",
    "            label_batch[i][i] = label\n",
    "\n",
    "        feed_dict = {\n",
    "            self.user_input: user_batch,\n",
    "            self.item_input: item_batch,\n",
    "            self.label: label_batch\n",
    "        }\n",
    "        if iter != 0:\n",
    "            [_optimize, _loss] = session.run(\n",
    "                [self.optimize, self.loss], feed_dict=feed_dict)\n",
    "        else:\n",
    "            writer = tf.summary.FileWriter(\"log\",session.graph)\n",
    "            [_loss] = session.run([self.loss], feed_dict=feed_dict)\n",
    "            writer.close()\n",
    "\n",
    "        return _loss\n",
    "                \n",
    "    def train(self,\n",
    "              nov_distri_path,\n",
    "              model_path,\n",
    "              beta=0.0,\n",
    "              batch_size=128,\n",
    "              learning_rate=0.006,\n",
    "              nu=0.0001,\n",
    "              embedding_size=600,\n",
    "              EVERY_N_ITERATIONS=50,\n",
    "              MAX_ITERATIONS=0,\n",
    "              predict_pair=[]):\n",
    "        self.rng=np.random.RandomState(SEED)\n",
    "        self.beta = beta\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.nu = nu\n",
    "        self.embedding_size = embedding_size\n",
    "        self.EVERY_N_ITERATIONS = EVERY_N_ITERATIONS\n",
    "        self.MAX_ITERATIONS = MAX_ITERATIONS\n",
    "\n",
    "        self.nov_distri_path = \"./\"+ nov_distri_path\n",
    "        self.model_path = \"./\"+ model_path\n",
    "        \n",
    "        self.pos_distri,self.all_distri=self.get_novelty_distribution()\n",
    "\n",
    "        \n",
    "        graph = tf.Graph()\n",
    "        dataset = self.util.dataset\n",
    "        with graph.as_default():\n",
    "            tf.set_random_seed(SEED)\n",
    "            with tf.name_scope(\"input\"):\n",
    "                self.user_input = tf.placeholder(\n",
    "                    tf.int32, shape=[None, dataset._USER_SIZE], name='user_info')\n",
    "                self.item_input = tf.placeholder(\n",
    "                    tf.int32, shape=[None, dataset._ITEM_SIZE], name='item_info')\n",
    "                self.label = tf.placeholder(\n",
    "                    tf.int32, shape=[None, None], name='label')\n",
    "                \n",
    "            with tf.name_scope(\"intercept\"):\n",
    "                 b = tf.Variable(\n",
    "                    initial_value=tf.truncated_normal(\n",
    "                        (self.embedding_size, 1),\n",
    "                        stddev=1.0 / np.sqrt(self.embedding_size)))\n",
    "\n",
    "            with tf.name_scope(\"user_embedding\"):\n",
    "                W = tf.Variable(\n",
    "                    initial_value=tf.truncated_normal(\n",
    "                        (self.embedding_size, dataset._USER_SIZE_BIN),\n",
    "                        stddev=1.0 / np.sqrt(self.embedding_size)))\n",
    "                w_offsets = [0] + [\n",
    "                    sum(dataset._USER_SIZE_OF_MASK_FIELDS[:i + 1])\n",
    "                    for i, j in enumerate(dataset._USER_SIZE_OF_MASK_FIELDS[:-1])\n",
    "                ]\n",
    "                w_offsets = tf.matmul(\n",
    "                    tf.ones(\n",
    "                        shape=(tf.shape(self.user_input)[0], 1), dtype=tf.int32),\n",
    "                    tf.convert_to_tensor([w_offsets]))\n",
    "                w_columns = self.user_input[:, :-dataset.\n",
    "                                            _USER_SIZE_ONLY_NUM] + w_offsets  # last column is not an index\n",
    "                w_selected = tf.gather(W, w_columns, axis=1)\n",
    "            # age * corresponding column of W\n",
    "            \n",
    "                aux = tf.matmul(\n",
    "                    W[:, -dataset._USER_SIZE_ONLY_NUM:],\n",
    "                    tf.transpose(\n",
    "                        tf.to_float(\n",
    "                            (self.user_input[:, -dataset._USER_SIZE_ONLY_NUM:]))))\n",
    "                user_batch_num = tf.reshape(\n",
    "                    aux,\n",
    "                    shape=(self.embedding_size, tf.shape(self.user_input)[0], 1))\n",
    "                w_with_num = tf.concat([w_selected, user_batch_num], axis=2)\n",
    "                w_result = tf.reduce_sum(w_with_num, axis=2)\n",
    "            with tf.name_scope(\"item_embedding\"):\n",
    "                A = tf.Variable(\n",
    "                    initial_value=tf.truncated_normal(\n",
    "                        (self.embedding_size, dataset._ITEM_SIZE_BIN),\n",
    "                        stddev=1.0 / np.sqrt(self.embedding_size)))\n",
    "                # select and sum the columns of A depending on the input\n",
    "                a_offsets = [0] + [\n",
    "                    sum(dataset._ITEM_SIZE_OF_MASK_FIELDS[:i + 1])\n",
    "                    for i, j in enumerate(dataset._ITEM_SIZE_OF_MASK_FIELDS[:-1])\n",
    "                ]\n",
    "                a_offsets = tf.matmul(\n",
    "                    tf.ones(\n",
    "                        shape=(tf.shape(self.item_input)[0], 1), dtype=tf.int32),\n",
    "                    tf.convert_to_tensor([a_offsets]))\n",
    "                a_columns = self.item_input[:, :-dataset.\n",
    "                                            _ITEM_SIZE_ONLY_NUM] + a_offsets  # last two columns are not indices\n",
    "                a_selected = tf.gather(A, a_columns, axis=1)\n",
    "                # dates * corresponding last two columns of A\n",
    "                aux = tf.matmul(\n",
    "                    A[:, -dataset._ITEM_SIZE_ONLY_NUM:],\n",
    "                    tf.transpose(\n",
    "                        tf.to_float(\n",
    "                            self.item_input[:, -dataset._ITEM_SIZE_ONLY_NUM:])))\n",
    "                item_batch_num = tf.reshape(\n",
    "                    aux,\n",
    "                    shape=(self.embedding_size, tf.shape(self.item_input)[0], 1))\n",
    "                # ... and the intercept\n",
    "                intercept = tf.gather(\n",
    "                    b,\n",
    "                    tf.zeros(\n",
    "                        shape=(tf.shape(self.item_input)[0], 1), dtype=tf.int32),\n",
    "                    axis=1)\n",
    "                a_with_num = tf.concat(\n",
    "                    [a_selected, item_batch_num, intercept], axis=2)\n",
    "                a_result = tf.reduce_sum(a_with_num, axis=2)\n",
    "\n",
    "            with tf.name_scope(\"output\"):\n",
    "                g = tf.matmul(tf.transpose(w_result), a_result)\n",
    "\n",
    "                x = tf.to_float(self.label) * g\n",
    "                self.prob = tf.nn.sigmoid(x)\n",
    "\n",
    "                loss = tf.reduce_mean(tf.nn.softplus(tf.diag_part(-x)))\n",
    "\n",
    "                # Regularization\n",
    "                reg = self.nu * (tf.nn.l2_loss(W) + tf.nn.l2_loss(A))\n",
    "                # Loss function with regularization (what we want to minimize)\n",
    "                loss_to_minimize = loss + reg\n",
    "                \n",
    "                self.loss= loss_to_minimize\n",
    "\n",
    "                self.optimize = tf.train.AdamOptimizer(\n",
    "                    learning_rate=self.learning_rate).minimize(\n",
    "                        loss=loss_to_minimize)\n",
    "        # Once thep graph is created, let's probgram the training loop\n",
    "        \n",
    "        config = tf.ConfigProto(allow_soft_placement=True)\n",
    "        config.gpu_options.allow_growth = True\n",
    "        \n",
    "        with tf.Session(config=config,graph=graph) as session:\n",
    "            tf.set_random_seed(SEED)\n",
    "            session.run(tf.global_variables_initializer())\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "            try:\n",
    "                saver.restore(session, self.model_path)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "            average_loss = 0.0\n",
    "            for iter in range(self.MAX_ITERATIONS + 1):\n",
    "                train_loss = self.train_a_batch(iter, session)\n",
    "                average_loss += train_loss\n",
    "                print('Iteration', iter, 'Train_loss', train_loss)\n",
    "\n",
    "                if iter % self.EVERY_N_ITERATIONS == 0:\n",
    "                    ndcg,reward0, reward1, agg_div, entro_div = self.eval_performance()\n",
    "                    saver.save(session, self.model_path)\n",
    "                        \n",
    "            result = {}\n",
    "            return result, ndcg, reward1, agg_div, entro_div\n",
    "         \n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "    def run_baseline(self,pref,dis_type,n_jobs=1):\n",
    "        if BASELINE==1:\n",
    "            s1=\"%s_nov_distri_beta%.1f\"%(pref,0.0)\n",
    "        if NPOS_FLG==1:\n",
    "            s1+=\"_npos\"\n",
    "        else:\n",
    "            s1+=\"_normal\"\n",
    "        s2=\"%s_K_600_beta_%.1f_vald2\"%(pref,0.0)\n",
    "        if NPOS_FLG==1:\n",
    "            s2+=\"_npos\"\n",
    "        else:\n",
    "            s2+=\"_normal\"\n",
    "        result,ndcg, reward1, agg_div, entro_div=self.train(\n",
    "            s1,s2,beta=0.0,predict_pair=[],MAX_ITERATIONS=0)\n",
    "\n",
    "        RESORT_LEN=500\n",
    "        tmp=self.base_recommend(self.prob_by_uitem, RESORT_LEN)\n",
    "        \n",
    "        sz=len(tmp)\n",
    "        \n",
    "        self.uid_to_dismat={}\n",
    "        for uid in tmp:\n",
    "            recommend=tmp[uid]\n",
    "            dis_mat=self.util.distant(recommend,recommend,is_test=0,dis_type=dis_type)\n",
    "            self.uid_to_dismat[uid]=dis_mat\n",
    "            #if uid==1:\n",
    "                #print(dis_mat)\n",
    "        result_list = []\n",
    "        k_list=[]\n",
    "        if dis_type==0:\n",
    "            k_list=[0.0,0.02,0.04,0.06,0.08,0.1,0.3,0.5,0.7,\n",
    "            0.9,1.0]\n",
    "        else:\n",
    "            k_list=[0.04,0.08,0.1,0.12,0.14,0.16,0.18,0.2,0.22,0.24]\n",
    "\n",
    "        def MMR(self,uid,k,dis_type):\n",
    "            R, S = self.base_uid_to_recomm[uid], []\n",
    "            #start=time.clock()\n",
    "            dis_mat=self.uid_to_dismat[uid]\n",
    "            #print('dis use time',time.clock()-start)\n",
    "            for index,itemid in enumerate(self.base_uid_to_recomm[uid]):\n",
    "                    R[index]=(itemid,index)\n",
    "            #print(R[:20])\n",
    "            #start=time.clock()\n",
    "            #print('precise recommend uid',uid)\n",
    "            for iter in range(self.top_N):\n",
    "                #print(iter)\n",
    "                fobj_set = []\n",
    "                for itemid_r,index_r in R:\n",
    "                    #print(itemid_r,index_r)\n",
    "                    rel = self.prob_by_uitem[uid][itemid_r]\n",
    "                    min_dist = 1.0\n",
    "                    for itemid_s,index_s in S:\n",
    "                        dist = dis_mat[index_r][index_s]\n",
    "                        min_dist = min(dist, min_dist)\n",
    "                    fobj = (1 - k) * rel + k * min_dist\n",
    "                    fobj_set.append((itemid_r, index_r,fobj))\n",
    "\n",
    "                pair = max(fobj_set, key=lambda x: x[2])\n",
    "                best = (pair[0],pair[1])\n",
    "                R.remove(best)\n",
    "                S.append(best)\n",
    "            #print('single recommend time',time.clock()-start)\n",
    "            return (uid,[x[0] for x in S])\n",
    "\n",
    "        for k in k_list:\n",
    "            print(\"lambda=%f\" % (k))\n",
    "            #start=time.clock()\n",
    "            self.base_uid_to_recomm=copy.deepcopy(tmp)\n",
    "            #print(self.base_uid_to_recomm[1],self.base_uid_to_recomm[2])\n",
    "            #print('use time',time.clock()-start)\n",
    "            start=time.clock()\n",
    "            results = Parallel(n_jobs=n_jobs,verbose=100,pre_dispatch='all',batch_size=int(len(dataset.list_uid)/n_jobs))(\n",
    "            delayed(MMR)(self,uid,k,dis_type) for uid in self.base_uid_to_recomm)\n",
    "            print('use time',time.clock()-start)\n",
    "            print('Baseline Performance')\n",
    "            ndcg, avg_reward0,avg_reward1, agg_div, enp_div = self.measure(\n",
    "                dict(results))\n",
    "            result_list.append((k, ndcg, avg_reward1, agg_div, enp_div))\n",
    "        pd.DataFrame(\n",
    "             result_list,\n",
    "             columns=[\n",
    "                 \"lambda\", \"ndcg\", \"avg_reward\", \"agg_div\", \"entropy_div\"\n",
    "             ]).to_csv(\n",
    "                 \"%s_baseline_result_\"%(pref) + str(SEED) + \"_eq%d_%s.csv\"%(14 if dis_type==1 else 15,\"normal\" if NPOS_FLG==0 else \"npos\" ), index=False)\n",
    "\n",
    "\n",
    "    def __init__(self, util):\n",
    "        self.top_N = 10\n",
    "        self.LIMIT = 100\n",
    "        self.util = util\n",
    "        self.dataset = util.dataset\n",
    "        self.NUM_USERS = len(self.util.dataset.df_userinfo)\n",
    "        self.NUM_ITEMS = len(self.util.dataset.df_iteminfo)\n",
    "        self.beta = 0.0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T08:44:06.997198Z",
     "start_time": "2018-11-27T08:42:44.151710Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta 0.0\n",
      "Calculating novelty distribution...\n",
      "INFO:tensorflow:Restoring parameters from ./ml_K_600_beta_0.0_vald2_npos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./ml_K_600_beta_0.0_vald2_npos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 Train_loss 0.8484484\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.1s finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\99701\\AppData\\Local\\Continuum\\anaconda3\\envs\\machine learning\\lib\\site-packages\\ipykernel_launcher.py:48: DeprecationWarning: elementwise != comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 4324 out of 4324 | elapsed:   11.1s finished\n",
      "Novelty: reward(β=0)=0.77613 reward(β=1)=0.44924\n",
      "Diversity: aggdiv=0.01462 entropydiv=6.51331\n",
      "Tue Nov 27 16:43:07 2018\n",
      "Calculating novelty distribution...\n",
      "INFO:tensorflow:Restoring parameters from ./ml_K_600_beta_0.0_vald2_npos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./ml_K_600_beta_0.0_vald2_npos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 Train_loss 0.8484484\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 4324 out of 4324 | elapsed:   11.2s finished\n",
      "Novelty: reward(β=0)=0.77613 reward(β=1)=0.44924\n",
      "Diversity: aggdiv=0.01462 entropydiv=6.51331\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-728e4eeffc09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"beta\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"ndcg\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"avg_reward\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"agg_div\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"entropy_div\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ml_newmethod_result_%s_%s.csv\"\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSEED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"normal\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mNPOS_FLG\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"npos\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masctime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mrec_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_baseline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpref\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-44-05a562a6a1ce>\u001b[0m in \u001b[0;36mrun_baseline\u001b[1;34m(self, pref, dis_type, n_jobs)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[0mRESORT_LEN\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m         \u001b[0mtmp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_recommend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprob_by_uitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRESORT_LEN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m         \u001b[0msz\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-05a562a6a1ce>\u001b[0m in \u001b[0;36mbase_recommend\u001b[1;34m(self, prob_by_uitem, top_N, n_jobs)\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[0mdelayed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecommend_single_user\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_rateditem_byuid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0muid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprob_by_uitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0muid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist_itemid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtop_N\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m             for uid in list(filter(\n\u001b[1;32m---> 89\u001b[1;33m                 lambda uid: len(self.dataset.test_positem_byuid[uid]) >= self.top_N,self.dataset.list_uid)))\n\u001b[0m\u001b[0;32m     90\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_uid_to_recomm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muid_to_recomm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_uid_to_recomm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\machine learning\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    981\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    982\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 983\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    984\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\machine learning\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    823\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 825\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    826\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\machine learning\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 782\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    783\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    784\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\machine learning\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\machine learning\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    543\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    546\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\machine learning\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    259\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 261\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\machine learning\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    259\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 261\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-05a562a6a1ce>\u001b[0m in \u001b[0;36mrecommend_single_user\u001b[1;34m(uid, rated, prob_lst, list_item, top_N)\u001b[0m\n\u001b[0;32m     78\u001b[0m                 \u001b[0mrel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpair\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mitemid\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrated\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m                     \u001b[0mrecomm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitemid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m                     \u001b[0mcnt\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mcnt\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtop_N\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "util=RecommendSysUtil(dataset)\n",
    "rec_sys=RecommendSys(util)\n",
    "\n",
    "beta_list=[0.0]#,0.5,1.0,1.5,2.0,5.0]#[ float(x) for x in FLAGS.beta.split(',')]\n",
    "max_iter=0 #int(FLAGS.iter)\n",
    "result_list=[]\n",
    "\n",
    "pref=\"ml\"\n",
    "\n",
    "\n",
    "for beta in list(beta_list):\n",
    "    print('beta',beta)\n",
    "    s1=\"%s_nov_distri\"%(pref)\n",
    "    if beta>=0.1 or beta == 0.0 :\n",
    "         s2=\"%s_K_600_beta_%.1f_vald2\"%(pref,beta)\n",
    "    else:\n",
    "         s2=\"%s_K_600_beta_%.3f_vald2\"%(pref,beta)\n",
    "    \n",
    "    if NPOS_FLG==1:\n",
    "        s1+=\"_npos\"\n",
    "        s2+=\"_npos\"\n",
    "    else:\n",
    "        s1+=\"_normal\"\n",
    "        s2+=\"_normal\"\n",
    "        \n",
    "    result,ndcg, reward1, agg_div, entro_div=rec_sys.train(\n",
    "    s1,s2,beta=beta, predict_pair=[],MAX_ITERATIONS=max_iter)\n",
    "    result_list.append((beta,ndcg,reward1,agg_div,entro_div))\n",
    "pd.DataFrame(result_list,columns=[\"beta\",\"ndcg\",\"avg_reward\",\"agg_div\",\"entropy_div\"]).to_csv(\"ml_newmethod_result_%s_%s.csv\"%(str(SEED),\"normal\" if NPOS_FLG==0 else \"npos\"),index=False)\n",
    "print(time.asctime())\n",
    "rec_sys.run_baseline(pref,0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "284.4px",
    "left": "1166px",
    "right": "20px",
    "top": "120px",
    "width": "351.4px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
