{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T02:44:41.469655Z",
     "start_time": "2018-06-14T02:44:30.194580Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dingyuyang/anaconda/envs/py/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import time\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from scipy import spatial\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import scipy\n",
    "import copy\n",
    "from tqdm import * \n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T02:44:41.585910Z",
     "start_time": "2018-06-14T02:44:41.483364Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--beta', type=str, default='',help='beta')\n",
    "# parser.add_argument('--iter', type=str, default=0,help='max iteration')\n",
    "# parser.add_argument('--dist', type=str, default='',help='distant type')\n",
    "# parser.add_argument('--nov', type=str, default='',help='novelty type')\n",
    "# parser.add_argument('--baseline', type=str, default=0,help='whether to run baseline')\n",
    "\n",
    "# FLAGS, _ = parser.parse_known_args()\n",
    "SEED=2000\n",
    "\n",
    "DATA_DIR='./ml-100k/'\n",
    "MODEL_DIR='./'\n",
    "\n",
    "DISTANT_TYPE=1#int(FLAGS.dist)\n",
    "NOVELTY_TYPE=0#int(FLAGS.nov)\n",
    "BASELINE=1#int(FLAGS.baseline)\n",
    "\n",
    "assert(DISTANT_TYPE==0 or DISTANT_TYPE==1)\n",
    "assert(NOVELTY_TYPE==0 or NOVELTY_TYPE==1)\n",
    "\n",
    "MLOBJ_PATH = 'ml_obj_%d.pkl'%(SEED)\n",
    "UTILOBJ_PATH='ml_util_%d_dis_%d.pkl'%(SEED,DISTANT_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T02:44:42.501306Z",
     "start_time": "2018-06-14T02:44:41.614776Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MovieLens:\n",
    "    def load_raw_data(self):\n",
    "        f=tf.gfile.Open(DATA_DIR + 'u.data',\"r\")\n",
    "        self.df_rating = pd.read_csv(\n",
    "            f,\n",
    "            sep='\\t',\n",
    "            names=['uid', 'itemid', 'rating', 'time'])\n",
    "        \n",
    "        f=tf.gfile.Open(DATA_DIR + 'u.user',\"r\")\n",
    "        self.df_userinfo = pd.read_csv(\n",
    "            f,\n",
    "            sep='|',\n",
    "            names=['uid', 'age', 'sex', 'occupation', 'zip_code'])\n",
    "        list_item_attr = [\n",
    "            'itemid', 'title', 'rel_date', 'video_rel_date', 'imdb_url',\n",
    "            \"unknown\", \"Action\", \"Adventure\", \"Animation\", \"Children's\",\n",
    "            \"Comedy\", \"Crime\", \"Documentary\", \"Drama\", \"Fantasy\", \"Film-Noir\",\n",
    "            \"Horror\", \"Musical\", \"Mystery\", \"Romance\", \"Sci-Fi\", \"Thriller\",\n",
    "            \"War\", \"Western\"\n",
    "        ]\n",
    "        f=tf.gfile.Open(DATA_DIR + 'u.item',\"r\")\n",
    "        self.df_iteminfo = pd.read_csv(\n",
    "            f,\n",
    "            sep='|',\n",
    "            names=list_item_attr)\n",
    "        self.df_userinfo = self.df_userinfo.fillna(0)\n",
    "        self.df_iteminfo = self.df_iteminfo.fillna(0)\n",
    "\n",
    "    def minmax_scaler(self, list_attr, df):\n",
    "        for attr in list_attr:\n",
    "            df[attr] = df[attr] - min(df[attr])\n",
    "    def feature_engineering(self):\n",
    "\n",
    "        ##iteminfo\n",
    "        df_all = self.df_iteminfo\n",
    "        df_date = df_all[\"rel_date\"]\n",
    "        df_date = pd.to_datetime(df_date)\n",
    "        df_all[\"year\"] = df_date.apply(lambda x: x.year)\n",
    "        df_all[\"month\"] = df_date.apply(lambda x: x.month)\n",
    "        df_all[\"day\"] = df_date.apply(lambda x: x.day)\n",
    "        df_all.drop(\n",
    "            [\"rel_date\", \"imdb_url\", \"video_rel_date\", \"title\"],\n",
    "            axis=1,\n",
    "            inplace=True)\n",
    "        self.minmax_scaler([\"year\", \"month\", \"day\"], df_all)\n",
    "        df_numeric = df_all.select_dtypes(exclude=['object'])\n",
    "        df_obj = df_all.select_dtypes(include=['object']).copy()\n",
    "        for c in df_obj:\n",
    "            df_obj[c] = (pd.factorize(df_obj[c])[0])\n",
    "        self.df_iteminfo = pd.concat([df_numeric, df_obj], axis=1)\n",
    "\n",
    "        df_all = self.df_userinfo\n",
    "        self.minmax_scaler([\"age\"], df_all)\n",
    "        df_numeric = df_all.select_dtypes(exclude=['object'])\n",
    "        df_obj = df_all.select_dtypes(include=['object']).copy()\n",
    "        for c in df_obj:\n",
    "            df_obj[c] = (pd.factorize(df_obj[c])[0])\n",
    "        self.df_userinfo = pd.concat([df_numeric, df_obj], axis=1)\n",
    "\n",
    "    def __init__(self):\n",
    "        self.rating_threshold = 3\n",
    "        self.load_raw_data()\n",
    "        self.df_iteminfo[\"itemid\"]=self.df_iteminfo[\"itemid\"]-1\n",
    "        self.df_userinfo[\"uid\"]=self.df_userinfo[\"uid\"]-1\n",
    "        self.df_rating[\"itemid\"]=self.df_rating[\"itemid\"]-1\n",
    "        self.df_rating[\"uid\"]=self.df_rating[\"uid\"]-1\n",
    "        self.feature_engineering()\n",
    "        \n",
    "        self.user_numerical_attr =  [\"age\"]\n",
    "        self.item_numerical_attr = [\"year\", \"month\", \"day\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T02:44:43.388067Z",
     "start_time": "2018-06-14T02:44:42.506174Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movielens=MovieLens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T02:54:44.010465Z",
     "start_time": "2018-06-14T02:54:17.307712Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NovResysClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self,\n",
    "                 NOVELTY_TYPE,\n",
    "                 DISTANT_TYPE,\n",
    "                 user_info=None,\n",
    "                 uid_ind=None,\n",
    "                 user_num_inds=None,\n",
    "                 item_info=None,\n",
    "                 itemid_ind=None,\n",
    "                 item_num_inds=None,\n",
    "                 rating_info=None,\n",
    "                 rating_threshold=3,\n",
    "                 test_size=0.3,\n",
    "                 random_state=2018):\n",
    "        self.NOVELTY_TYPE=NOVELTY_TYPE\n",
    "        self.DISTANT_TYPE=DISTANT_TYPE\n",
    "        self.user_info = user_info\n",
    "        self.uid_ind = uid_ind\n",
    "        self.user_num_inds = user_num_inds\n",
    "        self.item_info = item_info\n",
    "        self.itemid_ind = itemid_ind\n",
    "        self.item_num_inds = item_num_inds\n",
    "        self.rating_info = rating_info\n",
    "        self.rating_threshold = rating_threshold\n",
    "        self.ratio = test_size\n",
    "        self.seed = random_state\n",
    "        \n",
    "        for ind in range(np.shape(self.user_info)[1]):\n",
    "            le=LabelEncoder()\n",
    "            le.fit(self.user_info[:,ind])\n",
    "            if ind == uid_ind:\n",
    "                self.uid_le=le\n",
    "            self.user_info[:,ind]=le.transform(self.user_info[:,ind])\n",
    "            \n",
    "        for ind in range(np.shape(self.item_info)[1]):\n",
    "            le=LabelEncoder()\n",
    "            le.fit(self.item_info[:,ind])\n",
    "            if ind == itemid_ind:\n",
    "                self.itemid_le=le\n",
    "            self.item_info[:,ind]=le.transform(self.item_info[:,ind])\n",
    "            \n",
    "        self.rating_info[:,0]=self.uid_le.transform(self.rating_info[:,0])\n",
    "        self.rating_info[:,1]=self.itemid_le.transform(self.rating_info[:,1])\n",
    "        self.first_fit=1\n",
    "            \n",
    "    def _split_history(self,dic,ratio):\n",
    "        seed = self.seed\n",
    "        dic1 = {}\n",
    "        dic2 = {}\n",
    "        for ky in dic:\n",
    "            lst = dic[ky]\n",
    "            lenoflist = len(lst)\n",
    "            if lenoflist != 0:\n",
    "                random.Random(seed).shuffle(lst)\n",
    "                dic1[ky] = lst[:int(ratio * lenoflist)]\n",
    "                dic2[ky] = lst[int(ratio * lenoflist):]\n",
    "            else:\n",
    "                dic1[ky] = []\n",
    "                dic2[ky] = []\n",
    "        return dic1, dic2\n",
    "\n",
    "    def _merge_history(self,dic1, dic2):\n",
    "        return {ky: list(set(dic1[ky]) | set(dic2[ky])) for ky in dic1}\n",
    "\n",
    "    def _reverse_history(self,dict_byuid):\n",
    "        result = {itemid: [] for itemid in self.itemid_list}\n",
    "        for uid in dict_byuid:\n",
    "            for itemid in dict_byuid[uid]:\n",
    "                result[itemid].append(uid)\n",
    "        return result\n",
    "\n",
    "    def preprocess(self):\n",
    "\n",
    "        self.uid_list = self.user_info[:,self.uid_ind]\n",
    "        self.itemid_list = self.item_info[:,self.itemid_ind]\n",
    "\n",
    "        self.all_posuser_byitemid = {itemid: [] for itemid in self.itemid_list}\n",
    "        self.all_positem_byuid = {uid: [] for uid in self.uid_list}\n",
    "        self.all_neguser_byitemid = {itemid: [] for itemid in self.itemid_list}\n",
    "        self.all_negitem_byuid = {uid: [] for uid in self.uid_list}\n",
    "        sz1 = len(self.uid_list)\n",
    "        sz2 = len(self.itemid_list)\n",
    "        df_all = self.rating_info\n",
    "\n",
    "        sz = len(self.rating_info)\n",
    "\n",
    "        self.ratings_byitemid = [[0.0 for uid in self.uid_list]\n",
    "                                 for itemid in self.itemid_list]\n",
    "        \n",
    "        print('preprocess rating info...')\n",
    "        time.sleep(1)\n",
    "        for  row in tqdm(self.rating_info):\n",
    "\n",
    "            rating = row[2]\n",
    "            uid = int(row[0])\n",
    "            itemid = int(row[1])\n",
    "\n",
    "            self.ratings_byitemid[itemid][uid] = rating\n",
    "            #self.rating_bypair[uid][itemid] = rating\n",
    "            if rating > self.rating_threshold:\n",
    "                self.all_posuser_byitemid[itemid].append(uid)\n",
    "                self.all_positem_byuid[uid].append(itemid)\n",
    "            else:\n",
    "                self.all_neguser_byitemid[itemid].append(uid)\n",
    "                self.all_negitem_byuid[uid].append(itemid)\n",
    "        \n",
    "        print('preprocess rating info succeed.')\n",
    "\n",
    "        print('preprocess field info...')\n",
    "        self._USER_SIZE_ONLY_NUM = len(self.user_num_inds)\n",
    "        self._USER_SIZE_OF_FIELDS = []\n",
    "        for ind in range(np.shape(self.user_info)[1]):\n",
    "            if ind in self.user_num_inds:\n",
    "                self._USER_SIZE_OF_FIELDS.append(1)\n",
    "            else:\n",
    "                self._USER_SIZE_OF_FIELDS.append(\n",
    "                    len(np.unique(self.user_info[:,ind])))\n",
    "                \n",
    "        self._USER_SIZE = len(self._USER_SIZE_OF_FIELDS)\n",
    "        self._USER_SIZE_OF_MASK_FIELDS = self._USER_SIZE_OF_FIELDS[:-self.\n",
    "                                                                   _USER_SIZE_ONLY_NUM]\n",
    "        self._USER_SIZE_BIN = sum(self._USER_SIZE_OF_FIELDS)\n",
    "\n",
    "        self._ITEM_SIZE_ONLY_NUM = len(self.item_num_inds)\n",
    "\n",
    "        self._ITEM_SIZE_OF_FIELDS = []\n",
    "        for ind in range(np.shape(self.item_info)[1]):\n",
    "            if ind in self.item_num_inds:\n",
    "                self._ITEM_SIZE_OF_FIELDS.append(1)\n",
    "            else:\n",
    "                self._ITEM_SIZE_OF_FIELDS.append(\n",
    "                    len(np.unique(self.item_info[:,ind])))\n",
    "\n",
    "        self._ITEM_SIZE = len(self._ITEM_SIZE_OF_FIELDS)\n",
    "        self._ITEM_SIZE_OF_MASK_FIELDS = self._ITEM_SIZE_OF_FIELDS[:-self.\n",
    "                                                                   _ITEM_SIZE_ONLY_NUM]\n",
    "        self._ITEM_SIZE_BIN = sum(self._ITEM_SIZE_OF_FIELDS)\n",
    "        \n",
    "        print('preprocess field info succeed.')\n",
    "        \n",
    "        print('build index pools...')\n",
    "\n",
    "        self.train_positem_byuid, self.test_positem_byuid = self._split_history(\n",
    "            self.all_positem_byuid, self.ratio)\n",
    "\n",
    "        self.train_posuser_byitemid, self.test_posuser_byitemid = self._reverse_history(\n",
    "            self.train_positem_byuid), self._reverse_history(\n",
    "                self.test_positem_byuid)\n",
    "\n",
    "        self.train_negitem_byuid, self.test_negitem_byuid = self._split_history(\n",
    "            self.all_negitem_byuid, self.ratio)\n",
    "\n",
    "        self.train_neguser_byitemid, self.test_neguser_byitemid = self._reverse_history(\n",
    "            self.train_negitem_byuid), self._reverse_history(\n",
    "                self.test_negitem_byuid)\n",
    "\n",
    "        self.train_rateduser_byitemid = self._merge_history(\n",
    "            self.train_posuser_byitemid, self.train_neguser_byitemid)\n",
    "\n",
    "        self.test_rateduser_byitemid = self._merge_history(\n",
    "            self.test_posuser_byitemid, self.test_neguser_byitemid)\n",
    "\n",
    "        self.train_rateditem_byuid = self._merge_history(self.train_positem_byuid,\n",
    "                                                     self.train_negitem_byuid)\n",
    "\n",
    "        self.test_rateditem_byuid = self._merge_history(self.test_positem_byuid,\n",
    "                                                    self.test_negitem_byuid)\n",
    "\n",
    "        print('build index pools succeed')\n",
    "        \n",
    "        \n",
    "        print('reorder feature columns...')\n",
    "        for ind in range(np.shape(self.user_info)[1]):\n",
    "            if ind in self.user_num_inds:\n",
    "                tmp = self.user_info[:,ind].copy()\n",
    "                tmp=np.reshape(tmp,(-1,1))\n",
    "                self.user_info=np.delete(self.user_info,[ind], axis=1)\n",
    "                #print(np.shape(tmp),np.shape(self.user_info))\n",
    "                self.user_info=np.concatenate((self.user_info,tmp),axis=1)\n",
    "        for ind in range(np.shape(self.item_info)[1]):\n",
    "            if ind in self.item_num_inds:\n",
    "                tmp = self.item_info[:,ind].copy()\n",
    "                tmp=np.reshape(tmp,(-1,1))\n",
    "                self.item_info=np.delete(self.item_info,[ind], axis=1)\n",
    "                self.item_info=np.concatenate((self.item_info,tmp),axis=1)    \n",
    "        print('reorder feature columns succeed.')\n",
    "        \n",
    "    def _set_distant(self, i, j):\n",
    "        users_i = self.train_rateduser_byitemid[i]\n",
    "        users_j = self.train_rateduser_byitemid[j]\n",
    "        if (len(users_j) != 0):\n",
    "            return 1 - 1.0 * len(set(users_i) & set(users_j)) / len(users_j)\n",
    "        else:\n",
    "            return 1.0\n",
    "\n",
    "    def _cosine_distant(self, i, j):\n",
    "        vec_i = self.ratings_byitemid[i]\n",
    "        vec_j = self.ratings_byitemid[j]\n",
    "        return 1 - np.dot(vec_i, vec_j)\n",
    "\n",
    "    def _distant(self, i, j):\n",
    "        if self.DISTANT_TYPE == 0:\n",
    "            return self._set_distant(i, j)\n",
    "        else:\n",
    "            return self._cosine_distant(i, j)\n",
    "\n",
    "    def novelty(self, uid, item_i):\n",
    "        items_byu = self.train_rateditem_byuid[uid]\n",
    "        if self.NOVELTY_TYPE == 0:\n",
    "            return np.mean(\n",
    "                [self._distant_mat[item_i][item_j] for item_j in items_byu])\n",
    "        else:\n",
    "            return -np.log2(\n",
    "                len(self.train_rateduser_byitemid[item_i]\n",
    "                    ) / len(self.uid_list) + pow(10, -9))\n",
    "\n",
    "    def _item_vectorize(self, itemid):\n",
    "        return self.item_info[itemid]\n",
    "\n",
    "    def _user_vectorize(self, uid):\n",
    "        return self.user_info[uid]\n",
    "\n",
    "    def _save_vec(self):\n",
    "        self.uid_to_vec = {}\n",
    "        self.itemid_to_vec = {}\n",
    "        sz = len(self.uid_list)\n",
    "        print('precalculate user vector...')\n",
    "        time.sleep(0.5)\n",
    "        for uid in tqdm(self.uid_list):\n",
    "            self.uid_to_vec[uid] = self._user_vectorize(uid)\n",
    "        time.sleep(0.5)\n",
    "        print('precalculate user vector succeed.')\n",
    "        print('precalculate item vector...')\n",
    "        time.sleep(0.5)\n",
    "        sz = len(self.itemid_list)\n",
    "        for itemid in tqdm(self.itemid_list):\n",
    "            self.itemid_to_vec[itemid] = self._item_vectorize(itemid)\n",
    "        time.sleep(0.5)\n",
    "        print('precalculate item vector succeed.')   \n",
    "          \n",
    "    def _save_distance(self):\n",
    "        new_ratings_byitemid=[]\n",
    "        print('precalculate rating vector...')\n",
    "        time.sleep(0.5)\n",
    "        for itemid in tqdm(self.itemid_list):\n",
    "            vec=[0.0 for uid in self.uid_list]\n",
    "            for uid in self.train_rateduser_byitemid[itemid]:\n",
    "                vec[uid]=self.ratings_byitemid[itemid][uid]\n",
    "            vec=np.array(vec)+pow(10,-9)\n",
    "            new_ratings_byitemid.append(vec/ np.linalg.norm(vec))\n",
    "        time.sleep(0.5)\n",
    "        print('precalculate rating vector succeed')\n",
    "        \n",
    "        self.ratings_byitemid = new_ratings_byitemid\n",
    "        self._distant_mat=[]\n",
    "        print('precalculate distance...')\n",
    "        time.sleep(0.5)\n",
    "        for index_i,i in enumerate(tqdm(self.itemid_list)):\n",
    "            self._distant_mat.append([])\n",
    "            for index_j,j in enumerate(self.itemid_list):\n",
    "                if index_j>index_i:\n",
    "                    self._distant_mat[index_i].append(self._distant(index_i,index_j))\n",
    "                elif index_j==index_i:\n",
    "                    self._distant_mat[index_i].append(0)\n",
    "                else:\n",
    "                    self._distant_mat[index_i].append(self._distant_mat[index_j][index_i])\n",
    "        time.sleep(0.5)\n",
    "        print('precalculate distance succeed.')      \n",
    "    def precalculate(self):\n",
    "        self._save_vec()\n",
    "        self._save_distance()\n",
    "    \n",
    "    \n",
    "    def _get_novelty_distribution(self, u):\n",
    "        list_positemid = self.train_positem_byuid[u]\n",
    "        list_negitemid = self.train_negitem_byuid[u]\n",
    "        positem_novdistr = [\n",
    "            pow(self.novelty(u, itemid), self.beta)\n",
    "            for itemid in list_positemid\n",
    "        ]\n",
    "        negitem_novdistr = [1.0 for itemid in list_negitemid]\n",
    "        return positem_novdistr / np.sum(\n",
    "            positem_novdistr), negitem_novdistr / np.sum(negitem_novdistr)\n",
    "\n",
    "    def _load_distribution(self):\n",
    "        pos_distr=[[] for uid in self.uid_list]\n",
    "        neg_distr=[[] for uid in self.uid_list]\n",
    "        for uid in self.uid_list:\n",
    "            #print('load the novelty distribution of user', uid)\n",
    "            pos_distr[uid], neg_distr[\n",
    "                uid] = self._get_novelty_distribution(uid)\n",
    "        return pos_distr,neg_distr\n",
    "\n",
    "    def _predict_mat(self, uid_list, itemid_list):\n",
    "\n",
    "        user_batch = [self.uid_to_vec[uid] for uid in uid_list]\n",
    "\n",
    "        item_batch = []\n",
    "\n",
    "        for itemid in itemid_list:\n",
    "            item_batch.append(self.itemid_to_vec[itemid])\n",
    "\n",
    "        label_batch = [[1] * len(itemid_list) for uid in uid_list]\n",
    "        #print(np.shape(user_batch),np.shape(item_batch),np.shape(label_batch))\n",
    "        prob_matrix = self.prob.eval(\n",
    "            session=self.sess,\n",
    "            feed_dict={\n",
    "                self.user_input: user_batch,\n",
    "                self.item_input: item_batch,\n",
    "                self.label: label_batch\n",
    "            })\n",
    "\n",
    "        return prob_matrix\n",
    "\n",
    "    def _predict_mat_by_queue(self, uid_list, itemid_list):\n",
    "        sz = len(itemid_list)\n",
    "        batch_sz = 10000\n",
    "        bins = int(sz / batch_sz)\n",
    "        ret = []\n",
    "        for idx in range(bins):\n",
    "            #print('_predict_mat_by_queue %d/%d' % (idx, bins))\n",
    "            tmp = self._predict_mat(\n",
    "                uid_list, itemid_list[idx * batch_sz:(idx + 1) * batch_sz])\n",
    "            if ret != []:\n",
    "                ret = np.concatenate((ret, tmp), axis=1)\n",
    "            else:\n",
    "                ret = tmp\n",
    "\n",
    "        tmp = self._predict_mat(uid_list, itemid_list[bins * batch_sz:])\n",
    "\n",
    "        if ret != []:\n",
    "            ret = np.concatenate((ret, tmp), axis=1)\n",
    "        else:\n",
    "            ret = tmp\n",
    "        return ret\n",
    "\n",
    "    def eval_performance(self):\n",
    "\n",
    "        self.prob_by_uitem = self._predict_mat_by_queue(self.uid_list, self.itemid_list)\n",
    "\n",
    "        self.uid_to_recomm = self._base_recommend(self.prob_by_uitem,\n",
    "                                                 self.top_N)\n",
    "        #print(uid_list)\n",
    "        #print(uid_to_recomm)\n",
    "        acc = self._print_accuracy(self.uid_to_recomm, self.prob_by_uitem)\n",
    "        reward0, reward1, agg_div, entro_div = self._print_diversity(\n",
    "            self.uid_to_recomm)\n",
    "        return reward0, reward1, agg_div, entro_div\n",
    "\n",
    "    def _print_accuracy(self, uid_to_recomm, prob_by_uitem):\n",
    "        acc = 0\n",
    "        for uid in self.uid_list:\n",
    "            if len(self.test_positem_byuid[uid]) < self.top_N:\n",
    "                continue\n",
    "                #pass\n",
    "            positem_test = list(self.test_positem_byuid[uid])\n",
    "\n",
    "            if len(set(positem_test) & set(uid_to_recomm[uid])) != 0:\n",
    "                acc += 1\n",
    "        return acc / len(uid_to_recomm)\n",
    "\n",
    "    def _base_recommend(self, prob_by_uitem, top_N):\n",
    "        uid_to_recomm = {}\n",
    "        for uid in self.uid_list:\n",
    "            if len(self.test_positem_byuid[uid]) < self.top_N:\n",
    "                continue\n",
    "                #pass\n",
    "            prob_row = prob_by_uitem[uid]\n",
    "            prob_arr = list(zip(self.itemid_list, prob_row))\n",
    "            prob_arr = sorted(prob_arr, key=lambda d: -d[1])\n",
    "            cnt = 0\n",
    "            uid_to_recomm[uid] = []\n",
    "            for pair in prob_arr:\n",
    "                itemid = pair[0]\n",
    "                if itemid not in self.train_rateditem_byuid[uid]:\n",
    "                    uid_to_recomm[uid].append(itemid)\n",
    "                    cnt += 1\n",
    "                    if cnt == top_N:\n",
    "                        break\n",
    "        return uid_to_recomm\n",
    "\n",
    "    def _print_diversity(self, uid_to_recomm):\n",
    "        avg_reward0 = 0.0\n",
    "        avg_reward1 = 0.0\n",
    "        agg_div = 0.0\n",
    "        enp_div = 0.0\n",
    "\n",
    "        cnt = 0\n",
    "        for uid in uid_to_recomm:\n",
    "            reward0 = 0.0\n",
    "            reward1 = 0.0\n",
    "            for itemid in uid_to_recomm[uid]:\n",
    "                if (itemid in self.test_positem_byuid[uid]):\n",
    "                    nov = self.novelty(uid, itemid)\n",
    "                    if nov == np.inf or np == -np.inf:\n",
    "                        nov = 0\n",
    "                    if nov != 0:\n",
    "                        nov0 = pow(nov, 0)\n",
    "                        nov1 = pow(nov, 1)\n",
    "                        cnt += 1\n",
    "                    reward0 = max(reward0, nov0)\n",
    "                    reward1 = max(reward1, nov1)\n",
    "            avg_reward0 += reward0\n",
    "            avg_reward1 += reward1\n",
    "\n",
    "        if avg_reward0 != 0:\n",
    "            avg_reward0 /= len(uid_to_recomm)\n",
    "        if avg_reward1 != 0:\n",
    "            avg_reward1 /= cnt\n",
    "\n",
    "        recomm_set = set()\n",
    "        for uid in uid_to_recomm:\n",
    "            recomm_set = recomm_set | set(uid_to_recomm[uid])\n",
    "        agg_div = len(recomm_set) / len(uid_to_recomm) / self.top_N\n",
    "\n",
    "        itemid_to_recomuser = {}\n",
    "\n",
    "        for uid in uid_to_recomm:\n",
    "            for itemid in uid_to_recomm[uid]:\n",
    "                if itemid not in itemid_to_recomuser:\n",
    "                    itemid_to_recomuser[itemid] = 0\n",
    "                itemid_to_recomuser[itemid] += 1\n",
    "\n",
    "        s = 0\n",
    "        for itemid in itemid_to_recomuser:\n",
    "            s += itemid_to_recomuser[itemid]\n",
    "\n",
    "        for itemid in itemid_to_recomuser:\n",
    "            probb = itemid_to_recomuser[itemid] / s + pow(10, -9)\n",
    "            enp_div += -(np.log2(probb) * probb)\n",
    "\n",
    "        #print('over diver %f'%(time.time()-t1))\n",
    "        print(\n",
    "            'Diversity: accuracy=%.5f novelty=%.5f aggdiv=%.5f entropydiv=%.5f'\n",
    "            % (avg_reward0, avg_reward1, agg_div, enp_div))\n",
    "        return avg_reward0, avg_reward1, agg_div, enp_div\n",
    "\n",
    "    def _train_a_batch(self, iter):\n",
    "        \n",
    "        loss_all = 0\n",
    "\n",
    "        user_batch = []\n",
    "        item_batch = []\n",
    "        label_batch = []\n",
    "        list_positemid = []\n",
    "        uid_list = []\n",
    "        list_label = []\n",
    "        list_negitemid = []\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            uid = 0\n",
    "            while (True):\n",
    "                uid = self.rng.randint(1, self.NUM_USERS)\n",
    "                if ((uid in self.uid_list)\n",
    "                        and len(self.train_positem_byuid[uid]) != 0\n",
    "                        and len(self.train_negitem_byuid[uid]) != 0):\n",
    "                    break\n",
    "            uid_list.append(uid)\n",
    "            \n",
    "        for uid in uid_list:\n",
    "            pos_itemid = self.rng.choice(\n",
    "                self.train_positem_byuid[uid], p=self.pos_distr[uid])\n",
    "            list_positemid.append(pos_itemid)\n",
    "            list_label.append(1)\n",
    "            user_batch.append(self._user_vectorize(uid))\n",
    "            pos_itemvec = self._item_vectorize(pos_itemid)\n",
    "            item_batch.append(pos_itemvec)\n",
    "\n",
    "        prob_by_uitem = self._predict_mat(uid_list, list_positemid)\n",
    "\n",
    "        neg_itemset = set()\n",
    "        neg_index = {}\n",
    "        for uid in uid_list:\n",
    "            neg_itemset = neg_itemset | set(self.train_negitem_byuid[uid])\n",
    "        for index, neg_item in enumerate(neg_itemset):\n",
    "            neg_index[neg_item] = index\n",
    "        neg_itemset = list(neg_itemset)\n",
    "        neg_prob_by_uitem = self._predict_mat(uid_list, neg_itemset)\n",
    "\n",
    "        violator_cnt = 0\n",
    "        for i, uid in enumerate(uid_list):\n",
    "            neg_itemid = -1\n",
    "            pos_itemid = list_positemid[i]\n",
    "            pos_prob = prob_by_uitem[i][i]\n",
    "            for k in range(self.LIMIT):\n",
    "                neg_itemid = self.rng.choice(\n",
    "                    self.train_negitem_byuid[uid],\n",
    "                    p=self.neg_distr[uid])\n",
    "                neg_prob = neg_prob_by_uitem[i][neg_index[neg_itemid]]\n",
    "                if neg_prob >= pos_prob and neg_prob != 0:\n",
    "                    break\n",
    "                else:\n",
    "                    neg_itemid = -1\n",
    "\n",
    "            if neg_itemid != -1:\n",
    "                violator_cnt += 1\n",
    "                list_label.append(-1)\n",
    "                user_batch.append(self._user_vectorize(uid))\n",
    "                neg_itemvec = self._item_vectorize(neg_itemid)\n",
    "                item_batch.append(neg_itemvec)\n",
    "\n",
    "        label_batch = [[1] * len(user_batch) for j in range(len(user_batch))]\n",
    "        for i, label in enumerate(list_label):\n",
    "            label_batch[i][i] = label\n",
    "\n",
    "        #print(np.shape(user_batch),np.shape(item_batch))\n",
    "        feed_dict = {\n",
    "            self.user_input: user_batch,\n",
    "            self.item_input: item_batch,\n",
    "            self.label: label_batch\n",
    "        }\n",
    "        [_optimize, _loss] = self.sess.run(\n",
    "            [self.optimize, self.loss], feed_dict=feed_dict)\n",
    "        return _loss\n",
    "\n",
    "    def _cal_val_loss(self):\n",
    "        p=[]\n",
    "        q=[]\n",
    "        prob_by_uitem = self._predict_mat(self.uid_list, self.itemid_list)\n",
    "        for uid in self.uid_list:\n",
    "            for itemid in self.val_positem_byuid[uid]:\n",
    "                prob=prob_by_uitem[uid][itemid]\n",
    "                p.append(1)\n",
    "                q.append(prob)\n",
    "            for itemid in self.val_negitem_byuid[uid]:\n",
    "                prob=prob_by_uitem[uid][itemid]\n",
    "                p.append(0)\n",
    "                q.append(prob)\n",
    "        q=[x+1e-20 for x in q]\n",
    "        return scipy.stats.entropy(p, q)\n",
    "\n",
    "    def _es_train(self, es_stage):\n",
    "        \n",
    "        if es_stage==1:\n",
    "            self.train_positem_byuid_buffer=self.train_positem_byuid.copy()\n",
    "            self.train_negitem_byuid_buffer=self.train_negitem_byuid.copy()\n",
    "            \n",
    "            self.train_positem_byuid,self.val_positem_byuid=self._split_history(self.train_positem_byuid,5/7)\n",
    "            self.train_negitem_byuid,self.val_negitem_byuid=self._split_history(self.train_negitem_byuid,5/7)\n",
    "       \n",
    "        elif es_stage==2:\n",
    "            self.train_positem_byuid=self.train_positem_byuid\n",
    "            self.train_negitem_byuid=self.train_negitem_byuid\n",
    "            \n",
    "    def _build(self):\n",
    "        tf.set_random_seed(self.seed)\n",
    "        with tf.name_scope(\"input\"):\n",
    "            self.user_input = tf.placeholder(\n",
    "                tf.int32, shape=[None, self._USER_SIZE], name='user_info')\n",
    "            self.item_input = tf.placeholder(\n",
    "                tf.int32, shape=[None, self._ITEM_SIZE], name='item_info')\n",
    "            self.label = tf.placeholder(\n",
    "                tf.int32, shape=[None, None], name='label')\n",
    "\n",
    "        # Variables\n",
    "        # embedding for users\n",
    "\n",
    "        with tf.name_scope(\"intercept\"):\n",
    "             b = tf.Variable(\n",
    "                initial_value=tf.truncated_normal(\n",
    "                    (self.embedding_size, 1),\n",
    "                    stddev=1.0 / np.sqrt(self.embedding_size)))\n",
    "\n",
    "        # select and sum the columns of W depending on the input\n",
    "\n",
    "        with tf.name_scope(\"user_embedding\"):\n",
    "            W = tf.Variable(\n",
    "                initial_value=tf.truncated_normal(\n",
    "                    (self.embedding_size, self._USER_SIZE_BIN),\n",
    "                    stddev=1.0 / np.sqrt(self.embedding_size)))\n",
    "            # embedding for movies\n",
    "\n",
    "            # intercept\n",
    "\n",
    "            w_offsets = [0] + [\n",
    "                sum(self._USER_SIZE_OF_MASK_FIELDS[:i + 1])\n",
    "                for i, j in enumerate(self._USER_SIZE_OF_MASK_FIELDS[:-1])\n",
    "            ]\n",
    "            w_offsets = tf.matmul(\n",
    "                tf.ones(\n",
    "                    shape=(tf.shape(self.user_input)[0], 1), dtype=tf.int32),\n",
    "                tf.convert_to_tensor([w_offsets]))\n",
    "            w_columns = self.user_input[:, :-\n",
    "                                        self._USER_SIZE_ONLY_NUM] + w_offsets  # last column is not an index\n",
    "            w_selected = tf.gather(W, w_columns, axis=1)\n",
    "        # age * corresponding column of W\n",
    "\n",
    "            aux = tf.matmul(\n",
    "                W[:, -self._USER_SIZE_ONLY_NUM:],\n",
    "                tf.transpose(\n",
    "                    tf.to_float(\n",
    "                        (self.user_input[:, -self._USER_SIZE_ONLY_NUM:]))))\n",
    "            batch_age = tf.reshape(\n",
    "                aux,\n",
    "                shape=(self.embedding_size, tf.shape(self.user_input)[0], 1))\n",
    "            w_with_age = tf.concat([w_selected, batch_age], axis=2)\n",
    "            w_result = tf.reduce_sum(w_with_age, axis=2)\n",
    "        with tf.name_scope(\"item_embedding\"):\n",
    "            A = tf.Variable(\n",
    "                initial_value=tf.truncated_normal(\n",
    "                    (self.embedding_size, self._ITEM_SIZE_BIN),\n",
    "                    stddev=1.0 / np.sqrt(self.embedding_size)))\n",
    "            # select and sum the columns of A depending on the input\n",
    "            a_offsets = [0] + [\n",
    "                sum(self._ITEM_SIZE_OF_MASK_FIELDS[:i + 1])\n",
    "                for i, j in enumerate(self._ITEM_SIZE_OF_MASK_FIELDS[:-1])\n",
    "            ]\n",
    "            a_offsets = tf.matmul(\n",
    "                tf.ones(\n",
    "                    shape=(tf.shape(self.item_input)[0], 1), dtype=tf.int32),\n",
    "                tf.convert_to_tensor([a_offsets]))\n",
    "            a_columns = self.item_input[:, :-\n",
    "                                        self._ITEM_SIZE_ONLY_NUM] + a_offsets  # last two columns are not indices\n",
    "            a_selected = tf.gather(A, a_columns, axis=1)\n",
    "            # dates * corresponding last two columns of A\n",
    "            aux = tf.matmul(\n",
    "                A[:, -self._ITEM_SIZE_ONLY_NUM:],\n",
    "                tf.transpose(\n",
    "                    tf.to_float(\n",
    "                        self.item_input[:, -self._ITEM_SIZE_ONLY_NUM:])))\n",
    "            batch_dates = tf.reshape(\n",
    "                aux,\n",
    "                shape=(self.embedding_size, tf.shape(self.item_input)[0], 1))\n",
    "            # ... and the intercept\n",
    "            intercept = tf.gather(\n",
    "                b,\n",
    "                tf.zeros(\n",
    "                    shape=(tf.shape(self.item_input)[0], 1), dtype=tf.int32),\n",
    "                axis=1)\n",
    "            a_with_dates = tf.concat(\n",
    "                [a_selected, batch_dates, intercept], axis=2)\n",
    "            a_result = tf.reduce_sum(a_with_dates, axis=2)\n",
    "\n",
    "            # Definition of g (Eq. (14) in the paper g = <Wu, Vi> = u^T * W^T * V * i)\n",
    "        with tf.name_scope(\"output\"):\n",
    "\n",
    "            g = tf.matmul(tf.transpose(w_result), a_result,name=\"score\")\n",
    "\n",
    "            x = tf.to_float(self.label) * g\n",
    "            self.prob = tf.nn.sigmoid(x,name=\"prob\")\n",
    "\n",
    "            loss = tf.reduce_mean(tf.nn.softplus(tf.diag_part(-x)),name=\"loss\")\n",
    "\n",
    "            # Regularization\n",
    "            reg = self.nu * (tf.nn.l2_loss(W) + tf.nn.l2_loss(A))\n",
    "            # Loss function with regularization (what we want to minimize)\n",
    "            loss_to_minimize = loss + reg\n",
    "\n",
    "            self.loss= loss_to_minimize\n",
    "\n",
    "            self.optimize = tf.train.AdamOptimizer(\n",
    "                learning_rate=self.learning_rate).minimize(\n",
    "                    loss=loss_to_minimize)\n",
    "\n",
    "    def _train(self):\n",
    "        self.pos_distr,self.neg_distr=self._load_distribution()\n",
    "        gpu_options = tf.GPUOptions(visible_device_list='1')\n",
    "        config = tf.ConfigProto(gpu_options=gpu_options)\n",
    "        config.gpu_options.allow_growth = True\n",
    "        if self.first_fit==1:\n",
    "            self.sess=tf.Session(config=config)\n",
    "        tf.set_random_seed(self.seed)\n",
    "        if self.first_fit==1:\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "            self.first_fit=0\n",
    "        \n",
    "        if early_stop_method==None:\n",
    "            for e in range(self.epochs+1):\n",
    "                print('epochs %d'%(e))\n",
    "                if e!=0:\n",
    "                    for iter in range(int(np.ceil(len(self.uid_list)/self.batch_size))):\n",
    "                        train_loss = self._train_a_batch(iter)\n",
    "\n",
    "                        print('Iteration', iter, 'Train_loss', train_loss)\n",
    "                reward0, reward1, agg_div, entro_div = self.eval_performance(\n",
    "                        )\n",
    "        else:\n",
    "            best_epoch=0\n",
    "            self._es_train(1)\n",
    "            self.val_hist=[]\n",
    "            self.train_hist=[]\n",
    "            best_epoch=0\n",
    "            for e in range(self.epochs+1):\n",
    "                best_epoch=e\n",
    "                print('epochs %d'%(e))\n",
    "                average_loss=0.0\n",
    "                cnt=0\n",
    "                \n",
    "                if e!=0:\n",
    "                    for iter in range(int(np.ceil(len(self.uid_list)/self.batch_size))):\n",
    "                        train_loss = self._train_a_batch(iter)\n",
    "                        average_loss+=train_loss\n",
    "                        \n",
    "                        print('Iteration', iter, 'Train_loss', train_loss)\n",
    "                self.val_hist.append(self._cal_val_loss())\n",
    "                cnt=cnt if cnt !=0 else 1\n",
    "                self.train_hist.append(average_loss/cnt)\n",
    "                flg,best_epoch=self.early_stop_method(e,self.train_hist,self.val_hist)\n",
    "                if flg == 1 : \n",
    "                    break\n",
    "                reward0, reward1, agg_div, entro_div = self.eval_performance()\n",
    "            self._es_train(2)\n",
    "            for e in range(best_epoch+1):\n",
    "                best_epoch=e\n",
    "                print('epochs %d'%(e))\n",
    "                if e!=0:\n",
    "                    for iter in range(int(np.ceil(len(self.uid_list)/self.batch_size))):\n",
    "                        train_loss = self._train_a_batch(iter)\n",
    "                        print('Iteration', iter, 'Train_loss', train_loss)\n",
    "                reward0, reward1, agg_div, entro_div = self.eval_performance()\n",
    "            \n",
    "\n",
    "    \n",
    "    def fit(self,\n",
    "             novelty_importance=0.0,\n",
    "             batch_size=128,learning_rate=0.006,nu=0.0001,\n",
    "             embedding_size=600,epochs=0,\n",
    "             topk=10, limit=100,early_stop_method=None):\n",
    "    \n",
    "        self.beta = novelty_importance\n",
    "        self.rng=np.random.RandomState(SEED)\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.nu = nu\n",
    "        self.embedding_size = embedding_size\n",
    "        self.epochs=epochs\n",
    "        self.top_N = topk\n",
    "        self.LIMIT = limit\n",
    "        self.NUM_USERS=len(self.user_info)\n",
    "        self.NUM_ITEMS=len(self.item_info)\n",
    "        \n",
    "        if self.first_fit==1:\n",
    "            self._build()\n",
    "            \n",
    "        self._train()\n",
    "        \n",
    "    def recommend(self,user,top_N=10):\n",
    "        tmp = self._predict_mat_by_queue([user], self.itemid_list)\n",
    "        prob_arr = list(zip(self.itemid_list, tmp[0]))\n",
    "        prob_arr = sorted(prob_arr, key=lambda d: -d[1])\n",
    "        cnt = 0\n",
    "        recommend_lst=[]\n",
    "        for pair in prob_arr:\n",
    "            itemid = pair[0]\n",
    "            if itemid not in self.train_rateditem_byuid[user]:\n",
    "                recommend_lst.append(itemid)\n",
    "                cnt += 1\n",
    "                if cnt == top_N:\n",
    "                    break\n",
    "        return recommend_lst\n",
    "    def predict(self,predict_pair):\n",
    "        user_list = []\n",
    "        item_list = []\n",
    "        \n",
    "        user_index={}\n",
    "        item_index={}\n",
    "        for (idx,(user, item)) in enumerate(predict_pair):\n",
    "            \n",
    "            if user not in user_index:\n",
    "                user_index[user]=idx\n",
    "            if item not in item_index:\n",
    "                item_index[item]=idx\n",
    "                \n",
    "            user_list.append(user)\n",
    "            item_list.append(item)\n",
    "        tmp = self._predict_mat_by_queue(user_list, item_list)\n",
    "        result=[]\n",
    "        for (user,item) in predict_pair:\n",
    "            result.append(tmp[user_index[user]][item_index[item]])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T02:54:44.030275Z",
     "start_time": "2018-06-14T02:54:44.013587Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_num_inds=[]\n",
    "item_num_inds=[]\n",
    "for idx,feat in enumerate(movielens.df_userinfo.columns):\n",
    "    if feat in movielens.user_numerical_attr:\n",
    "        user_num_inds.append(idx)\n",
    "        \n",
    "for idx,feat in enumerate(movielens.df_iteminfo.columns):\n",
    "    if feat in movielens.item_numerical_attr:\n",
    "        item_num_inds.append(idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T02:54:44.281233Z",
     "start_time": "2018-06-14T02:54:44.033283Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resys=NovResysClassifier(0,0,\n",
    "                         movielens.df_userinfo.values,0,user_num_inds,\n",
    "                         movielens.df_iteminfo.values,0,item_num_inds,\n",
    "                        movielens.df_rating.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T02:54:46.172878Z",
     "start_time": "2018-06-14T02:54:44.283376Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocess rating info...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:00<00:00, 275944.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocess rating info succeed.\n",
      "preprocess field info...\n",
      "preprocess field info succeed.\n",
      "build index pools...\n",
      "build index pools succeed\n",
      "reorder feature columns...\n",
      "reorder feature columns succeed.\n"
     ]
    }
   ],
   "source": [
    "resys.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T02:55:00.719102Z",
     "start_time": "2018-06-14T02:54:46.175432Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precalculate user vector...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 943/943 [00:00<00:00, 562941.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precalculate user vector succeed.\n",
      "precalculate item vector...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1682/1682 [00:00<00:00, 515627.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precalculate item vector succeed.\n",
      "precalculate rating vector...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1682/1682 [00:00<00:00, 3826.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precalculate rating vector succeed\n",
      "precalculate distance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1682/1682 [00:10<00:00, 167.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precalculate distance succeed.\n"
     ]
    }
   ],
   "source": [
    "resys.precalculate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T03:12:39.224003Z",
     "start_time": "2018-06-14T02:55:51.031797Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 0\n",
      "Diversity: accuracy=0.60667 novelty=0.43499 aggdiv=0.00667 entropydiv=3.63968\n",
      "epochs 1\n",
      "Iteration 0 Train_loss 82.442726\n",
      "Iteration 1 Train_loss 35.90266\n",
      "Iteration 2 Train_loss 28.741331\n",
      "Iteration 3 Train_loss 84.85866\n",
      "Iteration 4 Train_loss 103.16062\n",
      "Iteration 5 Train_loss 64.54839\n",
      "Iteration 6 Train_loss 23.886084\n",
      "Iteration 7 Train_loss 24.411633\n",
      "Diversity: accuracy=0.59833 novelty=0.43376 aggdiv=0.01311 entropydiv=3.95444\n",
      "epochs 2\n",
      "Iteration 0 Train_loss 46.305305\n",
      "Iteration 1 Train_loss 52.718254\n",
      "Iteration 2 Train_loss 46.668385\n",
      "Iteration 3 Train_loss 33.947624\n",
      "Iteration 4 Train_loss 17.006338\n",
      "Iteration 5 Train_loss 12.894575\n",
      "Iteration 6 Train_loss 26.624191\n",
      "Iteration 7 Train_loss 34.319626\n",
      "Diversity: accuracy=0.20024 novelty=0.56237 aggdiv=0.01216 entropydiv=4.49846\n",
      "epochs 3\n",
      "Iteration 0 Train_loss 35.09721\n",
      "Iteration 1 Train_loss 21.889061\n",
      "Iteration 2 Train_loss 11.109469\n",
      "Iteration 3 Train_loss 14.869532\n",
      "Iteration 4 Train_loss 20.814737\n",
      "Iteration 5 Train_loss 23.374249\n",
      "Iteration 6 Train_loss 17.51627\n",
      "Iteration 7 Train_loss 10.631828\n",
      "Diversity: accuracy=0.36710 novelty=0.45117 aggdiv=0.01907 entropydiv=5.86186\n",
      "epochs 4\n",
      "Iteration 0 Train_loss 11.601892\n",
      "Iteration 1 Train_loss 16.321283\n",
      "Iteration 2 Train_loss 19.999464\n",
      "Iteration 3 Train_loss 16.851873\n",
      "Iteration 4 Train_loss 12.087028\n",
      "Iteration 5 Train_loss 8.957653\n",
      "Iteration 6 Train_loss 12.374574\n",
      "Iteration 7 Train_loss 14.478523\n",
      "Diversity: accuracy=0.59118 novelty=0.43708 aggdiv=0.02551 entropydiv=5.16524\n",
      "epochs 5\n",
      "Iteration 0 Train_loss 12.682334\n",
      "Iteration 1 Train_loss 9.75724\n",
      "Iteration 2 Train_loss 7.522264\n",
      "Iteration 3 Train_loss 9.215733\n",
      "Iteration 4 Train_loss 8.47745\n",
      "Iteration 5 Train_loss 9.512332\n",
      "Iteration 6 Train_loss 6.099131\n",
      "Iteration 7 Train_loss 5.4008455\n",
      "Diversity: accuracy=0.55662 novelty=0.45038 aggdiv=0.02574 entropydiv=6.22599\n",
      "epochs 6\n",
      "Iteration 0 Train_loss 8.662013\n",
      "Iteration 1 Train_loss 7.8533916\n",
      "Iteration 2 Train_loss 6.6017747\n",
      "Iteration 3 Train_loss 4.625719\n",
      "Iteration 4 Train_loss 7.123719\n",
      "Iteration 5 Train_loss 6.4673715\n",
      "Iteration 6 Train_loss 4.197827\n",
      "Iteration 7 Train_loss 5.5928802\n",
      "Diversity: accuracy=0.57211 novelty=0.44712 aggdiv=0.03111 entropydiv=6.47007\n",
      "epochs 7\n",
      "Iteration 0 Train_loss 6.7215915\n",
      "Iteration 1 Train_loss 4.944707\n",
      "Iteration 2 Train_loss 4.2032104\n",
      "Iteration 3 Train_loss 3.9205084\n",
      "Iteration 4 Train_loss 4.590764\n",
      "Iteration 5 Train_loss 4.0545235\n",
      "Iteration 6 Train_loss 3.6880362\n",
      "Iteration 7 Train_loss 4.8658376\n",
      "Diversity: accuracy=0.53635 novelty=0.44781 aggdiv=0.03111 entropydiv=6.53210\n",
      "epochs 8\n",
      "Iteration 0 Train_loss 4.614238\n",
      "Iteration 1 Train_loss 3.3413954\n",
      "Iteration 2 Train_loss 3.6762443\n",
      "Iteration 3 Train_loss 4.7159452\n",
      "Iteration 4 Train_loss 3.8659127\n",
      "Iteration 5 Train_loss 3.6172266\n",
      "Iteration 6 Train_loss 3.7518697\n",
      "Iteration 7 Train_loss 3.2379289\n",
      "Diversity: accuracy=0.42074 novelty=0.51125 aggdiv=0.03075 entropydiv=6.32410\n",
      "epochs 9\n",
      "Iteration 0 Train_loss 2.8818\n",
      "Iteration 1 Train_loss 3.6054754\n",
      "Iteration 2 Train_loss 3.3993616\n",
      "Iteration 3 Train_loss 2.9460905\n",
      "Iteration 4 Train_loss 2.6270144\n",
      "Iteration 5 Train_loss 3.0622573\n",
      "Iteration 6 Train_loss 2.517198\n",
      "Iteration 7 Train_loss 2.3495278\n",
      "Diversity: accuracy=0.37902 novelty=0.51617 aggdiv=0.02920 entropydiv=5.98763\n",
      "epochs 10\n",
      "Iteration 0 Train_loss 2.7861578\n",
      "Iteration 1 Train_loss 2.9525344\n",
      "Iteration 2 Train_loss 2.7745314\n",
      "Iteration 3 Train_loss 2.389295\n",
      "Iteration 4 Train_loss 2.2843616\n",
      "Iteration 5 Train_loss 2.2162905\n",
      "Iteration 6 Train_loss 2.4572237\n",
      "Iteration 7 Train_loss 2.4461339\n",
      "Diversity: accuracy=0.36830 novelty=0.54972 aggdiv=0.03218 entropydiv=6.43974\n",
      "epochs 11\n",
      "Iteration 0 Train_loss 2.4776921\n",
      "Iteration 1 Train_loss 2.063418\n",
      "Iteration 2 Train_loss 2.263996\n",
      "Iteration 3 Train_loss 2.3159597\n",
      "Iteration 4 Train_loss 2.1334527\n",
      "Iteration 5 Train_loss 2.388454\n",
      "Iteration 6 Train_loss 2.1127224\n",
      "Iteration 7 Train_loss 2.2370214\n",
      "Diversity: accuracy=0.39333 novelty=0.55412 aggdiv=0.03206 entropydiv=6.41202\n",
      "epochs 12\n",
      "Iteration 0 Train_loss 2.1749868\n",
      "Iteration 1 Train_loss 2.1272163\n",
      "Iteration 2 Train_loss 1.9673929\n",
      "Iteration 3 Train_loss 1.9412112\n",
      "Iteration 4 Train_loss 2.1345837\n",
      "Iteration 5 Train_loss 1.6974514\n",
      "Iteration 6 Train_loss 1.8457303\n",
      "Iteration 7 Train_loss 2.0267913\n",
      "Diversity: accuracy=0.34446 novelty=0.59790 aggdiv=0.02992 entropydiv=6.10565\n",
      "epochs 13\n",
      "Iteration 0 Train_loss 1.8617587\n",
      "Iteration 1 Train_loss 1.4190848\n",
      "Iteration 2 Train_loss 1.7973162\n",
      "Iteration 3 Train_loss 1.8189622\n",
      "Iteration 4 Train_loss 1.5757045\n",
      "Iteration 5 Train_loss 1.5249379\n",
      "Iteration 6 Train_loss 1.4326644\n",
      "Iteration 7 Train_loss 1.5634739\n",
      "Diversity: accuracy=0.33254 novelty=0.64060 aggdiv=0.03099 entropydiv=6.26518\n",
      "epochs 14\n",
      "Iteration 0 Train_loss 1.6310537\n",
      "Iteration 1 Train_loss 1.6309884\n",
      "Iteration 2 Train_loss 1.5894128\n",
      "Iteration 3 Train_loss 1.4879313\n",
      "Iteration 4 Train_loss 1.3894355\n",
      "Iteration 5 Train_loss 1.421825\n",
      "Iteration 6 Train_loss 1.5306808\n",
      "Iteration 7 Train_loss 1.785464\n",
      "Diversity: accuracy=0.29797 novelty=0.65425 aggdiv=0.02992 entropydiv=5.95305\n",
      "epochs 15\n",
      "Iteration 0 Train_loss 1.5920146\n",
      "Iteration 1 Train_loss 1.5001006\n",
      "Iteration 2 Train_loss 1.5062377\n",
      "Iteration 3 Train_loss 1.6428493\n",
      "Iteration 4 Train_loss 1.6290163\n",
      "Iteration 5 Train_loss 1.4043131\n",
      "Iteration 6 Train_loss 1.4044099\n",
      "Iteration 7 Train_loss 1.7548379\n",
      "Diversity: accuracy=0.28129 novelty=0.63871 aggdiv=0.02908 entropydiv=6.02481\n",
      "epochs 16\n",
      "Iteration 0 Train_loss 1.4143252\n",
      "Iteration 1 Train_loss 1.402459\n",
      "Iteration 2 Train_loss 1.6745456\n",
      "Iteration 3 Train_loss 1.4426548\n",
      "Iteration 4 Train_loss 1.452998\n",
      "Iteration 5 Train_loss 1.4378331\n",
      "Iteration 6 Train_loss 1.5529402\n",
      "Iteration 7 Train_loss 1.2533785\n",
      "Diversity: accuracy=0.25983 novelty=0.62648 aggdiv=0.03051 entropydiv=5.91656\n",
      "epochs 17\n",
      "Iteration 0 Train_loss 1.4619389\n",
      "Iteration 1 Train_loss 1.3843573\n",
      "Iteration 2 Train_loss 1.296773\n",
      "Iteration 3 Train_loss 1.3739667\n",
      "Iteration 4 Train_loss 1.4296111\n",
      "Iteration 5 Train_loss 1.3385286\n",
      "Iteration 6 Train_loss 1.3929517\n",
      "Iteration 7 Train_loss 1.4067726\n",
      "Diversity: accuracy=0.24791 novelty=0.64379 aggdiv=0.03135 entropydiv=5.81877\n",
      "epochs 18\n",
      "Iteration 0 Train_loss 1.3792882\n",
      "Iteration 1 Train_loss 1.4025365\n",
      "Iteration 2 Train_loss 1.452219\n",
      "Iteration 3 Train_loss 1.3478831\n",
      "Iteration 4 Train_loss 1.4457135\n",
      "Iteration 5 Train_loss 1.4601067\n",
      "Iteration 6 Train_loss 1.516922\n",
      "Iteration 7 Train_loss 1.7293313\n",
      "Diversity: accuracy=0.23600 novelty=0.65295 aggdiv=0.02813 entropydiv=6.02088\n",
      "epochs 19\n",
      "Iteration 0 Train_loss 1.1181977\n",
      "Iteration 1 Train_loss 1.5451045\n",
      "Iteration 2 Train_loss 1.6049532\n",
      "Iteration 3 Train_loss 1.4840144\n",
      "Iteration 4 Train_loss 1.4541788\n",
      "Iteration 5 Train_loss 1.6171579\n",
      "Iteration 6 Train_loss 1.2538294\n",
      "Iteration 7 Train_loss 1.4516392\n",
      "Diversity: accuracy=0.28248 novelty=0.63825 aggdiv=0.03039 entropydiv=6.21691\n",
      "epochs 20\n",
      "Iteration 0 Train_loss 1.1400459\n",
      "Iteration 1 Train_loss 1.4348739\n",
      "Iteration 2 Train_loss 1.3996806\n",
      "Iteration 3 Train_loss 1.3654807\n",
      "Iteration 4 Train_loss 1.3014876\n",
      "Iteration 5 Train_loss 1.3418331\n",
      "Iteration 6 Train_loss 1.0176009\n",
      "Iteration 7 Train_loss 1.3074515\n",
      "Diversity: accuracy=0.29201 novelty=0.61667 aggdiv=0.03123 entropydiv=6.00490\n",
      "epochs 21\n",
      "Iteration 0 Train_loss 1.2056667\n",
      "Iteration 1 Train_loss 1.1543198\n",
      "Iteration 2 Train_loss 1.3677988\n",
      "Iteration 3 Train_loss 1.1812485\n",
      "Iteration 4 Train_loss 1.061957\n",
      "Iteration 5 Train_loss 1.1517799\n",
      "Iteration 6 Train_loss 1.1877869\n",
      "Iteration 7 Train_loss 1.00236\n",
      "Diversity: accuracy=0.32658 novelty=0.63651 aggdiv=0.03337 entropydiv=6.33624\n",
      "epochs 22\n",
      "Iteration 0 Train_loss 1.126582\n",
      "Iteration 1 Train_loss 1.161606\n",
      "Iteration 2 Train_loss 0.96223915\n",
      "Iteration 3 Train_loss 1.191911\n",
      "Iteration 4 Train_loss 0.9979328\n",
      "Iteration 5 Train_loss 0.99010384\n",
      "Iteration 6 Train_loss 1.0478169\n",
      "Iteration 7 Train_loss 1.0575523\n",
      "Diversity: accuracy=0.31824 novelty=0.63661 aggdiv=0.03266 entropydiv=6.20512\n",
      "epochs 23\n",
      "Iteration 0 Train_loss 0.96698236\n",
      "Iteration 1 Train_loss 0.951312\n",
      "Iteration 2 Train_loss 1.143266\n",
      "Iteration 3 Train_loss 0.8875171\n",
      "Iteration 4 Train_loss 1.0429043\n",
      "Iteration 5 Train_loss 0.9762862\n",
      "Iteration 6 Train_loss 1.0263233\n",
      "Iteration 7 Train_loss 1.0289819\n",
      "Diversity: accuracy=0.31108 novelty=0.64279 aggdiv=0.03302 entropydiv=6.34353\n",
      "epochs 24\n",
      "Iteration 0 Train_loss 0.94507957\n",
      "Iteration 1 Train_loss 1.1440201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2 Train_loss 1.288829\n",
      "Iteration 3 Train_loss 1.0254773\n",
      "Iteration 4 Train_loss 1.0961728\n",
      "Iteration 5 Train_loss 1.146063\n",
      "Iteration 6 Train_loss 0.9437233\n",
      "Iteration 7 Train_loss 1.2341331\n",
      "Diversity: accuracy=0.32181 novelty=0.63412 aggdiv=0.03552 entropydiv=6.49559\n",
      "epochs 25\n",
      "Iteration 0 Train_loss 1.2068197\n",
      "Iteration 1 Train_loss 1.1361852\n",
      "Iteration 2 Train_loss 1.2230041\n",
      "Iteration 3 Train_loss 1.3554001\n",
      "Iteration 4 Train_loss 1.0577681\n",
      "Iteration 5 Train_loss 1.1585593\n",
      "Iteration 6 Train_loss 1.0881587\n",
      "Iteration 7 Train_loss 0.9321904\n",
      "Diversity: accuracy=0.31824 novelty=0.59917 aggdiv=0.03266 entropydiv=6.25926\n",
      "epochs 26\n",
      "Iteration 0 Train_loss 0.9135765\n",
      "Iteration 1 Train_loss 1.0980058\n",
      "Iteration 2 Train_loss 1.0918852\n",
      "Iteration 3 Train_loss 1.0853395\n",
      "Iteration 4 Train_loss 0.9254053\n",
      "Iteration 5 Train_loss 1.0114789\n",
      "Iteration 6 Train_loss 1.1181669\n",
      "Iteration 7 Train_loss 1.0380299\n",
      "Diversity: accuracy=0.30155 novelty=0.63887 aggdiv=0.03445 entropydiv=6.39706\n",
      "epochs 27\n",
      "Iteration 0 Train_loss 1.2574394\n",
      "Iteration 1 Train_loss 1.1169269\n",
      "Iteration 2 Train_loss 0.9905106\n",
      "Iteration 3 Train_loss 0.84538484\n",
      "Iteration 4 Train_loss 1.00677\n",
      "Iteration 5 Train_loss 1.1138866\n",
      "Iteration 6 Train_loss 0.9814792\n",
      "Iteration 7 Train_loss 0.78999996\n",
      "Diversity: accuracy=0.29797 novelty=0.61398 aggdiv=0.03421 entropydiv=6.20936\n",
      "epochs 28\n",
      "Iteration 0 Train_loss 1.0313843\n",
      "Iteration 1 Train_loss 0.9809175\n",
      "Iteration 2 Train_loss 0.9010542\n",
      "Iteration 3 Train_loss 0.9432089\n",
      "Iteration 4 Train_loss 0.9704809\n",
      "Iteration 5 Train_loss 0.9402603\n",
      "Iteration 6 Train_loss 0.9598627\n",
      "Iteration 7 Train_loss 0.87427646\n",
      "Diversity: accuracy=0.26698 novelty=0.63085 aggdiv=0.03838 entropydiv=6.25577\n",
      "epochs 29\n",
      "Iteration 0 Train_loss 0.8947404\n",
      "Iteration 1 Train_loss 0.884406\n",
      "Iteration 2 Train_loss 1.1371727\n",
      "Iteration 3 Train_loss 1.0127034\n",
      "Iteration 4 Train_loss 0.96932757\n",
      "Iteration 5 Train_loss 0.7627403\n",
      "Iteration 6 Train_loss 0.7417981\n",
      "Iteration 7 Train_loss 0.707852\n",
      "Diversity: accuracy=0.22884 novelty=0.71274 aggdiv=0.03564 entropydiv=6.15928\n",
      "epochs 30\n",
      "Iteration 0 Train_loss 0.7980736\n",
      "Iteration 1 Train_loss 0.7430551\n",
      "Iteration 2 Train_loss 0.8837135\n",
      "Iteration 3 Train_loss 0.9802461\n",
      "Iteration 4 Train_loss 0.95876795\n",
      "Iteration 5 Train_loss 0.79891944\n",
      "Iteration 6 Train_loss 0.98563516\n",
      "Iteration 7 Train_loss 0.9133448\n",
      "Diversity: accuracy=0.26579 novelty=0.65138 aggdiv=0.03683 entropydiv=6.04701\n",
      "epochs 31\n",
      "Iteration 0 Train_loss 1.1282178\n",
      "Iteration 1 Train_loss 0.8777777\n",
      "Iteration 2 Train_loss 0.89273775\n",
      "Iteration 3 Train_loss 0.8284487\n",
      "Iteration 4 Train_loss 0.9986751\n",
      "Iteration 5 Train_loss 0.82201064\n",
      "Iteration 6 Train_loss 0.94291663\n",
      "Iteration 7 Train_loss 0.8331617\n",
      "Diversity: accuracy=0.28248 novelty=0.62283 aggdiv=0.03671 entropydiv=6.18523\n",
      "epochs 32\n",
      "Iteration 0 Train_loss 0.839953\n",
      "Iteration 1 Train_loss 0.867954\n",
      "Iteration 2 Train_loss 0.77975786\n",
      "Iteration 3 Train_loss 0.8695363\n",
      "Iteration 4 Train_loss 0.8674059\n",
      "Iteration 5 Train_loss 0.7624958\n",
      "Iteration 6 Train_loss 0.81126136\n",
      "Iteration 7 Train_loss 0.77985704\n",
      "Diversity: accuracy=0.30274 novelty=0.60729 aggdiv=0.03647 entropydiv=6.29871\n",
      "epochs 33\n",
      "Iteration 0 Train_loss 0.8816307\n",
      "Iteration 1 Train_loss 0.79070055\n",
      "Iteration 2 Train_loss 0.6817654\n",
      "Iteration 3 Train_loss 0.7552439\n",
      "Iteration 4 Train_loss 0.8124522\n",
      "Iteration 5 Train_loss 0.7594086\n",
      "Iteration 6 Train_loss 0.7749745\n",
      "Iteration 7 Train_loss 0.8646054\n",
      "Diversity: accuracy=0.24315 novelty=0.63696 aggdiv=0.03659 entropydiv=6.15983\n",
      "epochs 34\n",
      "Iteration 0 Train_loss 0.82689875\n",
      "Iteration 1 Train_loss 0.7844629\n",
      "Iteration 2 Train_loss 0.74699086\n",
      "Iteration 3 Train_loss 0.9241907\n",
      "Iteration 4 Train_loss 0.7673945\n",
      "Iteration 5 Train_loss 0.84316546\n",
      "Iteration 6 Train_loss 0.8803596\n",
      "Iteration 7 Train_loss 0.78968096\n",
      "Diversity: accuracy=0.23719 novelty=0.68297 aggdiv=0.03671 entropydiv=6.06425\n",
      "epochs 35\n",
      "Iteration 0 Train_loss 0.7972173\n",
      "Iteration 1 Train_loss 0.73275536\n",
      "Iteration 2 Train_loss 1.1034328\n",
      "Iteration 3 Train_loss 0.6539986\n",
      "Iteration 4 Train_loss 0.82391804\n",
      "Iteration 5 Train_loss 0.7223452\n",
      "Iteration 6 Train_loss 0.6992917\n",
      "Iteration 7 Train_loss 0.8869474\n",
      "Diversity: accuracy=0.26579 novelty=0.65740 aggdiv=0.03790 entropydiv=6.42297\n",
      "epochs 36\n",
      "Iteration 0 Train_loss 1.0563933\n",
      "Iteration 1 Train_loss 0.8684163\n",
      "Iteration 2 Train_loss 0.86382055\n",
      "Iteration 3 Train_loss 0.6360272\n",
      "Iteration 4 Train_loss 0.71499807\n",
      "Iteration 5 Train_loss 0.69751847\n",
      "Iteration 6 Train_loss 0.840696\n",
      "Iteration 7 Train_loss 0.7598703\n",
      "Diversity: accuracy=0.26698 novelty=0.57354 aggdiv=0.03433 entropydiv=6.06134\n",
      "epochs 37\n",
      "Iteration 0 Train_loss 0.8058882\n",
      "Iteration 1 Train_loss 0.772375\n",
      "Iteration 2 Train_loss 0.7437142\n",
      "Iteration 3 Train_loss 0.95800936\n",
      "Iteration 4 Train_loss 0.8324986\n",
      "Iteration 5 Train_loss 0.74487174\n",
      "Iteration 6 Train_loss 0.64933\n",
      "Iteration 7 Train_loss 0.86589044\n",
      "Diversity: accuracy=0.25030 novelty=0.61163 aggdiv=0.03421 entropydiv=6.17319\n",
      "epochs 38\n",
      "Iteration 0 Train_loss 0.7398483\n",
      "Iteration 1 Train_loss 0.6544167\n",
      "Iteration 2 Train_loss 0.74866444\n",
      "Iteration 3 Train_loss 1.0297283\n",
      "Iteration 4 Train_loss 0.79660237\n",
      "Iteration 5 Train_loss 0.7642142\n",
      "Iteration 6 Train_loss 0.9470582\n",
      "Iteration 7 Train_loss 0.8750719\n",
      "Diversity: accuracy=0.28010 novelty=0.65186 aggdiv=0.03897 entropydiv=6.45685\n",
      "epochs 39\n",
      "Iteration 0 Train_loss 0.8905442\n",
      "Iteration 1 Train_loss 0.82356805\n",
      "Iteration 2 Train_loss 0.64638263\n",
      "Iteration 3 Train_loss 0.78531057\n",
      "Iteration 4 Train_loss 0.7530975\n",
      "Iteration 5 Train_loss 0.7968737\n",
      "Iteration 6 Train_loss 0.6493734\n",
      "Iteration 7 Train_loss 0.86214465\n",
      "Diversity: accuracy=0.31943 novelty=0.59954 aggdiv=0.04124 entropydiv=6.37082\n",
      "epochs 40\n",
      "Iteration 0 Train_loss 0.723122\n",
      "Iteration 1 Train_loss 0.79733926\n",
      "Iteration 2 Train_loss 0.74902266\n",
      "Iteration 3 Train_loss 0.6920117\n",
      "Iteration 4 Train_loss 0.74915963\n",
      "Iteration 5 Train_loss 0.64986306\n",
      "Iteration 6 Train_loss 0.74842227\n",
      "Iteration 7 Train_loss 0.6861093\n",
      "Diversity: accuracy=0.33611 novelty=0.60927 aggdiv=0.04005 entropydiv=6.49978\n",
      "epochs 41\n",
      "Iteration 0 Train_loss 0.70384705\n",
      "Iteration 1 Train_loss 0.7647789\n",
      "Iteration 2 Train_loss 0.8787203\n",
      "Iteration 3 Train_loss 0.8624239\n",
      "Iteration 4 Train_loss 0.72762173\n",
      "Iteration 5 Train_loss 0.61271214\n",
      "Iteration 6 Train_loss 0.7405597\n",
      "Iteration 7 Train_loss 0.6966677\n",
      "Diversity: accuracy=0.41716 novelty=0.57500 aggdiv=0.04398 entropydiv=6.64513\n",
      "epochs 42\n",
      "Iteration 0 Train_loss 0.7220627\n",
      "Iteration 1 Train_loss 0.65691656\n",
      "Iteration 2 Train_loss 0.78486085\n",
      "Iteration 3 Train_loss 0.7220106\n",
      "Iteration 4 Train_loss 0.76143944\n",
      "Iteration 5 Train_loss 0.7250496\n",
      "Iteration 6 Train_loss 0.75175685\n",
      "Iteration 7 Train_loss 0.8368249\n",
      "Diversity: accuracy=0.36710 novelty=0.53106 aggdiv=0.04291 entropydiv=6.48894\n",
      "epochs 43\n",
      "Iteration 0 Train_loss 0.72532403\n",
      "Iteration 1 Train_loss 0.78345364\n",
      "Iteration 2 Train_loss 0.77207226\n",
      "Iteration 3 Train_loss 0.6063006\n",
      "Iteration 4 Train_loss 0.77774\n",
      "Iteration 5 Train_loss 0.819673\n",
      "Iteration 6 Train_loss 0.6894674\n",
      "Iteration 7 Train_loss 0.6890547\n",
      "Diversity: accuracy=0.33373 novelty=0.56348 aggdiv=0.04398 entropydiv=6.55273\n",
      "epochs 44\n",
      "Iteration 0 Train_loss 0.8183578\n",
      "Iteration 1 Train_loss 0.7274858\n",
      "Iteration 2 Train_loss 0.6559199\n",
      "Iteration 3 Train_loss 0.78540075\n",
      "Iteration 4 Train_loss 0.71492237\n",
      "Iteration 5 Train_loss 0.54207945\n",
      "Iteration 6 Train_loss 0.7507602\n",
      "Iteration 7 Train_loss 0.7480138\n",
      "Diversity: accuracy=0.32539 novelty=0.59563 aggdiv=0.04565 entropydiv=6.72009\n",
      "epochs 45\n",
      "Iteration 0 Train_loss 0.63332033\n",
      "Iteration 1 Train_loss 0.72160494\n",
      "Iteration 2 Train_loss 0.8239622\n",
      "Iteration 3 Train_loss 0.5554609\n",
      "Iteration 4 Train_loss 0.8508294\n",
      "Iteration 5 Train_loss 0.70516986\n",
      "Iteration 6 Train_loss 0.6500694\n",
      "Iteration 7 Train_loss 0.59695643\n",
      "Diversity: accuracy=0.34684 novelty=0.56606 aggdiv=0.04708 entropydiv=6.82853\n",
      "epochs 46\n",
      "Iteration 0 Train_loss 0.75254273\n",
      "Iteration 1 Train_loss 0.70257455\n",
      "Iteration 2 Train_loss 0.5447424\n",
      "Iteration 3 Train_loss 0.6669103\n",
      "Iteration 4 Train_loss 0.66134197\n",
      "Iteration 5 Train_loss 0.6044172\n",
      "Iteration 6 Train_loss 0.64252144\n",
      "Iteration 7 Train_loss 0.6332705\n",
      "Diversity: accuracy=0.34803 novelty=0.58231 aggdiv=0.04851 entropydiv=6.85121\n",
      "epochs 47\n",
      "Iteration 0 Train_loss 0.61784244\n",
      "Iteration 1 Train_loss 0.5286733\n",
      "Iteration 2 Train_loss 0.71368116\n",
      "Iteration 3 Train_loss 0.6262804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4 Train_loss 0.5553616\n",
      "Iteration 5 Train_loss 0.68721163\n",
      "Iteration 6 Train_loss 0.557643\n",
      "Iteration 7 Train_loss 0.62881786\n",
      "Diversity: accuracy=0.33731 novelty=0.56936 aggdiv=0.04768 entropydiv=6.77290\n",
      "epochs 48\n",
      "Iteration 0 Train_loss 0.6272099\n",
      "Iteration 1 Train_loss 0.63186264\n",
      "Iteration 2 Train_loss 0.6465392\n",
      "Iteration 3 Train_loss 0.6005597\n",
      "Iteration 4 Train_loss 0.686483\n",
      "Iteration 5 Train_loss 0.79063696\n",
      "Iteration 6 Train_loss 0.6594568\n",
      "Iteration 7 Train_loss 0.56241035\n",
      "Diversity: accuracy=0.37664 novelty=0.56150 aggdiv=0.04648 entropydiv=6.81264\n",
      "epochs 49\n",
      "Iteration 0 Train_loss 0.5711028\n",
      "Iteration 1 Train_loss 0.6130244\n",
      "Iteration 2 Train_loss 0.6048196\n",
      "Iteration 3 Train_loss 0.5176424\n",
      "Iteration 4 Train_loss 0.6529386\n",
      "Iteration 5 Train_loss 0.4720971\n",
      "Iteration 6 Train_loss 0.647465\n",
      "Iteration 7 Train_loss 0.6518659\n",
      "Diversity: accuracy=0.37545 novelty=0.52146 aggdiv=0.04696 entropydiv=6.75840\n",
      "epochs 50\n",
      "Iteration 0 Train_loss 0.6048489\n",
      "Iteration 1 Train_loss 0.6829831\n",
      "Iteration 2 Train_loss 0.6856215\n",
      "Iteration 3 Train_loss 0.49090382\n",
      "Iteration 4 Train_loss 0.72209793\n",
      "Iteration 5 Train_loss 0.5964862\n",
      "Iteration 6 Train_loss 0.6059342\n",
      "Iteration 7 Train_loss 0.6109921\n",
      "Diversity: accuracy=0.35042 novelty=0.54366 aggdiv=0.04708 entropydiv=6.74856\n",
      "epochs 51\n",
      "Iteration 0 Train_loss 0.59089357\n",
      "Iteration 1 Train_loss 0.546078\n",
      "Iteration 2 Train_loss 0.6140201\n",
      "Iteration 3 Train_loss 0.70633924\n",
      "Iteration 4 Train_loss 0.5374229\n",
      "Iteration 5 Train_loss 0.6991651\n",
      "Iteration 6 Train_loss 0.5876689\n",
      "Iteration 7 Train_loss 0.5196819\n",
      "Diversity: accuracy=0.36830 novelty=0.53561 aggdiv=0.04899 entropydiv=6.74352\n",
      "epochs 52\n",
      "Iteration 0 Train_loss 0.7246507\n",
      "Iteration 1 Train_loss 0.61358416\n",
      "Iteration 2 Train_loss 0.58055234\n",
      "Iteration 3 Train_loss 0.565284\n",
      "Iteration 4 Train_loss 0.66961414\n",
      "Iteration 5 Train_loss 0.59751576\n",
      "Iteration 6 Train_loss 0.6921838\n",
      "Iteration 7 Train_loss 0.5390534\n",
      "Diversity: accuracy=0.34923 novelty=0.60238 aggdiv=0.04779 entropydiv=6.82623\n",
      "epochs 53\n",
      "Iteration 0 Train_loss 0.54360974\n",
      "Iteration 1 Train_loss 0.5592836\n",
      "Iteration 2 Train_loss 0.5186714\n",
      "Iteration 3 Train_loss 0.5338948\n",
      "Iteration 4 Train_loss 0.58377373\n",
      "Iteration 5 Train_loss 0.5891066\n",
      "Iteration 6 Train_loss 0.6435765\n",
      "Iteration 7 Train_loss 0.55295694\n",
      "Diversity: accuracy=0.37187 novelty=0.54069 aggdiv=0.04648 entropydiv=6.77463\n",
      "epochs 54\n",
      "Iteration 0 Train_loss 0.59799176\n",
      "Iteration 1 Train_loss 0.56809866\n",
      "Iteration 2 Train_loss 0.48082155\n",
      "Iteration 3 Train_loss 0.61969364\n",
      "Iteration 4 Train_loss 0.5282867\n",
      "Iteration 5 Train_loss 0.49233776\n",
      "Iteration 6 Train_loss 0.5716326\n",
      "Iteration 7 Train_loss 0.62703437\n",
      "Diversity: accuracy=0.38379 novelty=0.54047 aggdiv=0.04625 entropydiv=6.72853\n",
      "epochs 55\n",
      "Iteration 0 Train_loss 0.5540328\n",
      "Iteration 1 Train_loss 0.5073503\n",
      "Iteration 2 Train_loss 0.5228735\n",
      "Iteration 3 Train_loss 0.5399724\n",
      "Iteration 4 Train_loss 0.4846015\n",
      "Iteration 5 Train_loss 0.5181534\n",
      "Iteration 6 Train_loss 0.59219956\n",
      "Iteration 7 Train_loss 0.56655955\n",
      "Diversity: accuracy=0.39333 novelty=0.54321 aggdiv=0.04899 entropydiv=6.84400\n",
      "epochs 56\n",
      "Iteration 0 Train_loss 0.53285134\n",
      "Iteration 1 Train_loss 0.48960155\n",
      "Iteration 2 Train_loss 0.46285868\n",
      "Iteration 3 Train_loss 0.55112714\n",
      "Iteration 4 Train_loss 0.5896922\n",
      "Iteration 5 Train_loss 0.4860372\n",
      "Iteration 6 Train_loss 0.6065369\n",
      "Iteration 7 Train_loss 0.4691648\n",
      "Diversity: accuracy=0.42670 novelty=0.52875 aggdiv=0.04791 entropydiv=6.84636\n",
      "epochs 57\n",
      "Iteration 0 Train_loss 0.57802105\n",
      "Iteration 1 Train_loss 0.53325075\n",
      "Iteration 2 Train_loss 0.5280802\n",
      "Iteration 3 Train_loss 0.5823342\n",
      "Iteration 4 Train_loss 0.5993476\n",
      "Iteration 5 Train_loss 0.48455608\n",
      "Iteration 6 Train_loss 0.5002681\n",
      "Iteration 7 Train_loss 0.55398774\n",
      "Diversity: accuracy=0.41597 novelty=0.52463 aggdiv=0.04601 entropydiv=6.83446\n",
      "epochs 58\n",
      "Iteration 0 Train_loss 0.56267035\n",
      "Iteration 1 Train_loss 0.5536387\n",
      "Iteration 2 Train_loss 0.44567144\n",
      "Iteration 3 Train_loss 0.45414513\n",
      "Iteration 4 Train_loss 0.53859746\n",
      "Iteration 5 Train_loss 0.5384404\n",
      "Iteration 6 Train_loss 0.53143066\n",
      "Iteration 7 Train_loss 0.46831357\n",
      "Diversity: accuracy=0.42431 novelty=0.52326 aggdiv=0.04732 entropydiv=6.87034\n",
      "epochs 59\n",
      "Iteration 0 Train_loss 0.5825417\n",
      "Iteration 1 Train_loss 0.70652485\n",
      "Iteration 2 Train_loss 0.5230314\n",
      "Iteration 3 Train_loss 0.54946977\n",
      "Iteration 4 Train_loss 0.45731688\n",
      "Iteration 5 Train_loss 0.39299428\n",
      "Iteration 6 Train_loss 0.5113112\n",
      "Iteration 7 Train_loss 0.52764136\n",
      "Diversity: accuracy=0.42074 novelty=0.51146 aggdiv=0.04911 entropydiv=6.78454\n",
      "epochs 60\n",
      "Iteration 0 Train_loss 0.52068293\n",
      "Iteration 1 Train_loss 0.5200498\n",
      "Iteration 2 Train_loss 0.6102727\n",
      "Iteration 3 Train_loss 0.6952727\n",
      "Iteration 4 Train_loss 0.56241107\n",
      "Iteration 5 Train_loss 0.40571427\n",
      "Iteration 6 Train_loss 0.5215021\n",
      "Iteration 7 Train_loss 0.6997729\n",
      "Diversity: accuracy=0.42670 novelty=0.56623 aggdiv=0.04923 entropydiv=6.93476\n",
      "epochs 61\n",
      "Iteration 0 Train_loss 0.52747816\n",
      "Iteration 1 Train_loss 0.6047566\n",
      "Iteration 2 Train_loss 0.5819181\n",
      "Iteration 3 Train_loss 0.60369295\n",
      "Iteration 4 Train_loss 0.52923405\n",
      "Iteration 5 Train_loss 0.43112677\n",
      "Iteration 6 Train_loss 0.53637207\n",
      "Iteration 7 Train_loss 0.6495788\n",
      "Diversity: accuracy=0.41120 novelty=0.57435 aggdiv=0.05030 entropydiv=6.93151\n",
      "epochs 62\n",
      "Iteration 0 Train_loss 0.5215697\n",
      "Iteration 1 Train_loss 0.56068593\n",
      "Iteration 2 Train_loss 0.47570378\n",
      "Iteration 3 Train_loss 0.5718205\n",
      "Iteration 4 Train_loss 0.5607734\n",
      "Iteration 5 Train_loss 0.6105986\n",
      "Iteration 6 Train_loss 0.6049652\n",
      "Iteration 7 Train_loss 0.5119952\n",
      "Diversity: accuracy=0.43147 novelty=0.56924 aggdiv=0.05066 entropydiv=6.92721\n",
      "epochs 63\n",
      "Iteration 0 Train_loss 0.5272218\n",
      "Iteration 1 Train_loss 0.47197276\n",
      "Iteration 2 Train_loss 0.5362537\n",
      "Iteration 3 Train_loss 0.44424683\n",
      "Iteration 4 Train_loss 0.6842785\n",
      "Iteration 5 Train_loss 0.5220771\n",
      "Iteration 6 Train_loss 0.56199896\n",
      "Iteration 7 Train_loss 0.5671188\n",
      "Diversity: accuracy=0.41597 novelty=0.53414 aggdiv=0.05197 entropydiv=6.96942\n",
      "epochs 64\n",
      "Iteration 0 Train_loss 0.45211697\n",
      "Iteration 1 Train_loss 0.48409235\n",
      "Iteration 2 Train_loss 0.4483677\n",
      "Iteration 3 Train_loss 0.49867922\n",
      "Iteration 4 Train_loss 0.51612514\n",
      "Iteration 5 Train_loss 0.56072116\n",
      "Iteration 6 Train_loss 0.39786008\n",
      "Iteration 7 Train_loss 0.5195195\n",
      "Diversity: accuracy=0.40644 novelty=0.52414 aggdiv=0.05030 entropydiv=6.90077\n",
      "epochs 65\n",
      "Iteration 0 Train_loss 0.4948972\n",
      "Iteration 1 Train_loss 0.5316562\n",
      "Iteration 2 Train_loss 0.51841515\n",
      "Iteration 3 Train_loss 0.5020911\n",
      "Iteration 4 Train_loss 0.5616242\n",
      "Iteration 5 Train_loss 0.48238188\n",
      "Iteration 6 Train_loss 0.37326264\n",
      "Iteration 7 Train_loss 0.5087452\n",
      "Diversity: accuracy=0.45888 novelty=0.52304 aggdiv=0.04946 entropydiv=6.82706\n",
      "epochs 66\n",
      "Iteration 0 Train_loss 0.43463644\n",
      "Iteration 1 Train_loss 0.50404316\n",
      "Iteration 2 Train_loss 0.524871\n",
      "Iteration 3 Train_loss 0.4752459\n",
      "Iteration 4 Train_loss 0.5120139\n",
      "Iteration 5 Train_loss 0.5286329\n",
      "Iteration 6 Train_loss 0.51219773\n",
      "Iteration 7 Train_loss 0.4859916\n",
      "Diversity: accuracy=0.44100 novelty=0.53777 aggdiv=0.05030 entropydiv=6.84147\n",
      "epochs 67\n",
      "Iteration 0 Train_loss 0.6565047\n",
      "Iteration 1 Train_loss 0.4805802\n",
      "Iteration 2 Train_loss 0.56217253\n",
      "Iteration 3 Train_loss 0.5570615\n",
      "Iteration 4 Train_loss 0.51588255\n",
      "Iteration 5 Train_loss 0.51311404\n",
      "Iteration 6 Train_loss 0.4147877\n",
      "Iteration 7 Train_loss 0.45802736\n",
      "Diversity: accuracy=0.45054 novelty=0.51159 aggdiv=0.05232 entropydiv=6.90246\n",
      "epochs 68\n",
      "Iteration 0 Train_loss 0.47985435\n",
      "Iteration 1 Train_loss 0.58306414\n",
      "Iteration 2 Train_loss 0.48403138\n",
      "Iteration 3 Train_loss 0.5849327\n",
      "Iteration 4 Train_loss 0.5457802\n",
      "Iteration 5 Train_loss 0.47616488\n",
      "Iteration 6 Train_loss 0.46789435\n",
      "Iteration 7 Train_loss 0.4403106\n",
      "Diversity: accuracy=0.47437 novelty=0.50362 aggdiv=0.05221 entropydiv=7.02129\n",
      "epochs 69\n",
      "Iteration 0 Train_loss 0.47682065\n",
      "Iteration 1 Train_loss 0.49037555\n",
      "Iteration 2 Train_loss 0.4584654\n",
      "Iteration 3 Train_loss 0.5323176\n",
      "Iteration 4 Train_loss 0.411761\n",
      "Iteration 5 Train_loss 0.41775453\n",
      "Iteration 6 Train_loss 0.5257501\n",
      "Iteration 7 Train_loss 0.60610145\n",
      "Diversity: accuracy=0.48510 novelty=0.51003 aggdiv=0.05304 entropydiv=6.91362\n",
      "epochs 70\n",
      "Iteration 0 Train_loss 0.49370706\n",
      "Iteration 1 Train_loss 0.45875543\n",
      "Iteration 2 Train_loss 0.4979071\n",
      "Iteration 3 Train_loss 0.49914488\n",
      "Iteration 4 Train_loss 0.39605975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5 Train_loss 0.49636176\n",
      "Iteration 6 Train_loss 0.4583401\n",
      "Iteration 7 Train_loss 0.52795887\n",
      "Diversity: accuracy=0.47437 novelty=0.50690 aggdiv=0.05423 entropydiv=6.89246\n",
      "epochs 71\n",
      "Iteration 0 Train_loss 0.43830687\n",
      "Iteration 1 Train_loss 0.37987038\n",
      "Iteration 2 Train_loss 0.53475213\n",
      "Iteration 3 Train_loss 0.3952843\n",
      "Iteration 4 Train_loss 0.46873853\n",
      "Iteration 5 Train_loss 0.5551112\n",
      "Iteration 6 Train_loss 0.51949763\n",
      "Iteration 7 Train_loss 0.48307014\n",
      "Diversity: accuracy=0.46961 novelty=0.48618 aggdiv=0.05375 entropydiv=6.92561\n",
      "epochs 72\n",
      "Iteration 0 Train_loss 0.5225311\n",
      "Iteration 1 Train_loss 0.46452984\n",
      "Iteration 2 Train_loss 0.5241344\n",
      "Iteration 3 Train_loss 0.45007843\n",
      "Iteration 4 Train_loss 0.57104677\n",
      "Iteration 5 Train_loss 0.44347382\n",
      "Iteration 6 Train_loss 0.40251613\n",
      "Iteration 7 Train_loss 0.519613\n",
      "Diversity: accuracy=0.48510 novelty=0.47671 aggdiv=0.05387 entropydiv=6.97016\n",
      "epochs 73\n",
      "Iteration 0 Train_loss 0.46081114\n",
      "Iteration 1 Train_loss 0.5326743\n",
      "Iteration 2 Train_loss 0.4594317\n",
      "Iteration 3 Train_loss 0.41082823\n",
      "Iteration 4 Train_loss 0.48964673\n",
      "Iteration 5 Train_loss 0.40718403\n",
      "Iteration 6 Train_loss 0.40936986\n",
      "Iteration 7 Train_loss 0.41286057\n",
      "Diversity: accuracy=0.47795 novelty=0.49902 aggdiv=0.05268 entropydiv=7.01512\n",
      "epochs 74\n",
      "Iteration 0 Train_loss 0.39020362\n",
      "Iteration 1 Train_loss 0.45458537\n",
      "Iteration 2 Train_loss 0.4536808\n",
      "Iteration 3 Train_loss 0.33316043\n",
      "Iteration 4 Train_loss 0.5318929\n",
      "Iteration 5 Train_loss 0.4132825\n",
      "Iteration 6 Train_loss 0.4792139\n",
      "Iteration 7 Train_loss 0.4538986\n",
      "Diversity: accuracy=0.50775 novelty=0.46739 aggdiv=0.05471 entropydiv=6.91320\n",
      "epochs 75\n",
      "Iteration 0 Train_loss 0.54380566\n",
      "Iteration 1 Train_loss 0.4315495\n",
      "Iteration 2 Train_loss 0.42639247\n",
      "Iteration 3 Train_loss 0.42013282\n",
      "Iteration 4 Train_loss 0.4140551\n",
      "Iteration 5 Train_loss 0.46682858\n",
      "Iteration 6 Train_loss 0.5193661\n",
      "Iteration 7 Train_loss 0.40264386\n",
      "Diversity: accuracy=0.48272 novelty=0.49349 aggdiv=0.05030 entropydiv=6.96798\n",
      "epochs 76\n",
      "Iteration 0 Train_loss 0.4454332\n",
      "Iteration 1 Train_loss 0.53910214\n",
      "Iteration 2 Train_loss 0.40125537\n",
      "Iteration 3 Train_loss 0.4925691\n",
      "Iteration 4 Train_loss 0.45396787\n",
      "Iteration 5 Train_loss 0.42013332\n",
      "Iteration 6 Train_loss 0.46751633\n",
      "Iteration 7 Train_loss 0.50392723\n",
      "Diversity: accuracy=0.50298 novelty=0.49397 aggdiv=0.05221 entropydiv=6.95807\n",
      "epochs 77\n",
      "Iteration 0 Train_loss 0.49066174\n",
      "Iteration 1 Train_loss 0.42980167\n",
      "Iteration 2 Train_loss 0.4068184\n",
      "Iteration 3 Train_loss 0.4298362\n",
      "Iteration 4 Train_loss 0.43502235\n",
      "Iteration 5 Train_loss 0.45370138\n",
      "Iteration 6 Train_loss 0.5149657\n",
      "Iteration 7 Train_loss 0.48519516\n",
      "Diversity: accuracy=0.51490 novelty=0.47953 aggdiv=0.05221 entropydiv=6.96734\n",
      "epochs 78\n",
      "Iteration 0 Train_loss 0.54778075\n",
      "Iteration 1 Train_loss 0.49593362\n",
      "Iteration 2 Train_loss 0.51266074\n",
      "Iteration 3 Train_loss 0.5326785\n",
      "Iteration 4 Train_loss 0.44327658\n",
      "Iteration 5 Train_loss 0.463987\n",
      "Iteration 6 Train_loss 0.490875\n",
      "Iteration 7 Train_loss 0.42464083\n",
      "Diversity: accuracy=0.51967 novelty=0.48156 aggdiv=0.05340 entropydiv=7.06909\n",
      "epochs 79\n",
      "Iteration 0 Train_loss 0.5350968\n",
      "Iteration 1 Train_loss 0.49062398\n",
      "Iteration 2 Train_loss 0.4783007\n",
      "Iteration 3 Train_loss 0.4552383\n",
      "Iteration 4 Train_loss 0.46366078\n",
      "Iteration 5 Train_loss 0.46762455\n",
      "Iteration 6 Train_loss 0.49741173\n",
      "Iteration 7 Train_loss 0.42470503\n",
      "Diversity: accuracy=0.49106 novelty=0.49427 aggdiv=0.05411 entropydiv=7.06532\n",
      "epochs 80\n",
      "Iteration 0 Train_loss 0.5428537\n",
      "Iteration 1 Train_loss 0.48457807\n",
      "Iteration 2 Train_loss 0.49863946\n",
      "Iteration 3 Train_loss 0.51061773\n",
      "Iteration 4 Train_loss 0.44673693\n",
      "Iteration 5 Train_loss 0.46852762\n",
      "Iteration 6 Train_loss 0.51105374\n",
      "Iteration 7 Train_loss 0.47553295\n",
      "Diversity: accuracy=0.48629 novelty=0.48848 aggdiv=0.05268 entropydiv=6.96099\n",
      "epochs 81\n",
      "Iteration 0 Train_loss 0.49704862\n",
      "Iteration 1 Train_loss 0.3895545\n",
      "Iteration 2 Train_loss 0.4287262\n",
      "Iteration 3 Train_loss 0.49301374\n",
      "Iteration 4 Train_loss 0.46157682\n",
      "Iteration 5 Train_loss 0.5324349\n",
      "Iteration 6 Train_loss 0.45810607\n",
      "Iteration 7 Train_loss 0.4954489\n",
      "Diversity: accuracy=0.48033 novelty=0.51101 aggdiv=0.05328 entropydiv=7.06097\n",
      "epochs 82\n",
      "Iteration 0 Train_loss 0.5121986\n",
      "Iteration 1 Train_loss 0.40591425\n",
      "Iteration 2 Train_loss 0.39019948\n",
      "Iteration 3 Train_loss 0.4631114\n",
      "Iteration 4 Train_loss 0.48228896\n",
      "Iteration 5 Train_loss 0.42102087\n",
      "Iteration 6 Train_loss 0.4245625\n",
      "Iteration 7 Train_loss 0.51761734\n",
      "Diversity: accuracy=0.48987 novelty=0.47352 aggdiv=0.05256 entropydiv=7.10199\n",
      "epochs 83\n",
      "Iteration 0 Train_loss 0.3401096\n",
      "Iteration 1 Train_loss 0.44413075\n",
      "Iteration 2 Train_loss 0.49153948\n",
      "Iteration 3 Train_loss 0.4077996\n",
      "Iteration 4 Train_loss 0.51154554\n",
      "Iteration 5 Train_loss 0.41179758\n",
      "Iteration 6 Train_loss 0.4776237\n",
      "Iteration 7 Train_loss 0.5027157\n",
      "Diversity: accuracy=0.48749 novelty=0.51012 aggdiv=0.05113 entropydiv=7.01406\n",
      "epochs 84\n",
      "Iteration 0 Train_loss 0.4801935\n",
      "Iteration 1 Train_loss 0.5334923\n",
      "Iteration 2 Train_loss 0.41721892\n",
      "Iteration 3 Train_loss 0.37377375\n",
      "Iteration 4 Train_loss 0.46305966\n",
      "Iteration 5 Train_loss 0.43913698\n",
      "Iteration 6 Train_loss 0.45133203\n",
      "Iteration 7 Train_loss 0.45125303\n",
      "Diversity: accuracy=0.53278 novelty=0.48102 aggdiv=0.05435 entropydiv=6.97585\n",
      "epochs 85\n",
      "Iteration 0 Train_loss 0.5179689\n",
      "Iteration 1 Train_loss 0.43452066\n",
      "Iteration 2 Train_loss 0.4702696\n",
      "Iteration 3 Train_loss 0.49903244\n",
      "Iteration 4 Train_loss 0.48535556\n",
      "Iteration 5 Train_loss 0.43864515\n",
      "Iteration 6 Train_loss 0.507078\n",
      "Iteration 7 Train_loss 0.4269992\n",
      "Diversity: accuracy=0.53039 novelty=0.48757 aggdiv=0.05435 entropydiv=7.08498\n",
      "epochs 86\n",
      "Iteration 0 Train_loss 0.4556285\n",
      "Iteration 1 Train_loss 0.4965244\n",
      "Iteration 2 Train_loss 0.46008545\n",
      "Iteration 3 Train_loss 0.43171087\n",
      "Iteration 4 Train_loss 0.45000157\n",
      "Iteration 5 Train_loss 0.46241647\n",
      "Iteration 6 Train_loss 0.5312223\n",
      "Iteration 7 Train_loss 0.3669275\n",
      "Diversity: accuracy=0.50060 novelty=0.45200 aggdiv=0.05542 entropydiv=7.09059\n",
      "epochs 87\n",
      "Iteration 0 Train_loss 0.51915354\n",
      "Iteration 1 Train_loss 0.44902092\n",
      "Iteration 2 Train_loss 0.43280554\n",
      "Iteration 3 Train_loss 0.38735324\n",
      "Iteration 4 Train_loss 0.37715012\n",
      "Iteration 5 Train_loss 0.4893791\n",
      "Iteration 6 Train_loss 0.38722807\n",
      "Iteration 7 Train_loss 0.54380655\n",
      "Diversity: accuracy=0.46603 novelty=0.48775 aggdiv=0.05495 entropydiv=7.02620\n",
      "epochs 88\n",
      "Iteration 0 Train_loss 0.47945094\n",
      "Iteration 1 Train_loss 0.44360328\n",
      "Iteration 2 Train_loss 0.44008073\n",
      "Iteration 3 Train_loss 0.50266385\n",
      "Iteration 4 Train_loss 0.4552192\n",
      "Iteration 5 Train_loss 0.36577046\n",
      "Iteration 6 Train_loss 0.4244084\n",
      "Iteration 7 Train_loss 0.4391654\n",
      "Diversity: accuracy=0.47676 novelty=0.45786 aggdiv=0.05685 entropydiv=7.01691\n",
      "epochs 89\n",
      "Iteration 0 Train_loss 0.51658946\n",
      "Iteration 1 Train_loss 0.47921205\n",
      "Iteration 2 Train_loss 0.44683462\n",
      "Iteration 3 Train_loss 0.44364086\n",
      "Iteration 4 Train_loss 0.47884446\n",
      "Iteration 5 Train_loss 0.3889266\n",
      "Iteration 6 Train_loss 0.41334954\n",
      "Iteration 7 Train_loss 0.61283785\n",
      "Diversity: accuracy=0.49344 novelty=0.49874 aggdiv=0.05518 entropydiv=7.01950\n",
      "epochs 90\n",
      "Iteration 0 Train_loss 0.3863423\n",
      "Iteration 1 Train_loss 0.41065928\n",
      "Iteration 2 Train_loss 0.4902391\n",
      "Iteration 3 Train_loss 0.3826005\n",
      "Iteration 4 Train_loss 0.41065583\n",
      "Iteration 5 Train_loss 0.4548865\n",
      "Iteration 6 Train_loss 0.48680153\n",
      "Iteration 7 Train_loss 0.39595807\n",
      "Diversity: accuracy=0.52205 novelty=0.47487 aggdiv=0.05578 entropydiv=7.05241\n",
      "epochs 91\n",
      "Iteration 0 Train_loss 0.5340299\n",
      "Iteration 1 Train_loss 0.40732822\n",
      "Iteration 2 Train_loss 0.43872124\n",
      "Iteration 3 Train_loss 0.39520097\n",
      "Iteration 4 Train_loss 0.4807374\n",
      "Iteration 5 Train_loss 0.45767307\n",
      "Iteration 6 Train_loss 0.47282863\n",
      "Iteration 7 Train_loss 0.48671576\n",
      "Diversity: accuracy=0.49464 novelty=0.49167 aggdiv=0.05638 entropydiv=7.02012\n",
      "epochs 92\n",
      "Iteration 0 Train_loss 0.4172285\n",
      "Iteration 1 Train_loss 0.35162902\n",
      "Iteration 2 Train_loss 0.456994\n",
      "Iteration 3 Train_loss 0.42372435\n",
      "Iteration 4 Train_loss 0.4192148\n",
      "Iteration 5 Train_loss 0.38647893\n",
      "Iteration 6 Train_loss 0.4295438\n",
      "Iteration 7 Train_loss 0.4077584\n",
      "Diversity: accuracy=0.52443 novelty=0.47544 aggdiv=0.05328 entropydiv=6.95418\n",
      "epochs 93\n",
      "Iteration 0 Train_loss 0.47780025\n",
      "Iteration 1 Train_loss 0.44876826\n",
      "Iteration 2 Train_loss 0.4726318\n",
      "Iteration 3 Train_loss 0.4462559\n",
      "Iteration 4 Train_loss 0.39475852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5 Train_loss 0.435176\n",
      "Iteration 6 Train_loss 0.30066335\n",
      "Iteration 7 Train_loss 0.44947207\n",
      "Diversity: accuracy=0.52324 novelty=0.46867 aggdiv=0.05530 entropydiv=7.08679\n",
      "epochs 94\n",
      "Iteration 0 Train_loss 0.34882364\n",
      "Iteration 1 Train_loss 0.43022656\n",
      "Iteration 2 Train_loss 0.51680464\n",
      "Iteration 3 Train_loss 0.3911125\n",
      "Iteration 4 Train_loss 0.49357206\n",
      "Iteration 5 Train_loss 0.40020478\n",
      "Iteration 6 Train_loss 0.37767068\n",
      "Iteration 7 Train_loss 0.43110308\n",
      "Diversity: accuracy=0.53159 novelty=0.46692 aggdiv=0.05507 entropydiv=7.11360\n",
      "epochs 95\n",
      "Iteration 0 Train_loss 0.49048266\n",
      "Iteration 1 Train_loss 0.3944574\n",
      "Iteration 2 Train_loss 0.51298857\n",
      "Iteration 3 Train_loss 0.42139572\n",
      "Iteration 4 Train_loss 0.30183303\n",
      "Iteration 5 Train_loss 0.4690469\n",
      "Iteration 6 Train_loss 0.39839274\n",
      "Iteration 7 Train_loss 0.42826676\n",
      "Diversity: accuracy=0.54231 novelty=0.44987 aggdiv=0.05423 entropydiv=7.03301\n",
      "epochs 96\n",
      "Iteration 0 Train_loss 0.48967758\n",
      "Iteration 1 Train_loss 0.4484626\n",
      "Iteration 2 Train_loss 0.36878872\n",
      "Iteration 3 Train_loss 0.40147614\n",
      "Iteration 4 Train_loss 0.4577641\n",
      "Iteration 5 Train_loss 0.4780048\n",
      "Iteration 6 Train_loss 0.44764456\n",
      "Iteration 7 Train_loss 0.46127653\n",
      "Diversity: accuracy=0.52563 novelty=0.45517 aggdiv=0.05518 entropydiv=6.98092\n",
      "epochs 97\n",
      "Iteration 0 Train_loss 0.5251701\n",
      "Iteration 1 Train_loss 0.45886654\n",
      "Iteration 2 Train_loss 0.36832452\n",
      "Iteration 3 Train_loss 0.4330966\n",
      "Iteration 4 Train_loss 0.3617165\n",
      "Iteration 5 Train_loss 0.43111706\n",
      "Iteration 6 Train_loss 0.4151188\n",
      "Iteration 7 Train_loss 0.4058747\n",
      "Diversity: accuracy=0.55542 novelty=0.46488 aggdiv=0.05602 entropydiv=7.11987\n",
      "epochs 98\n",
      "Iteration 0 Train_loss 0.37920564\n",
      "Iteration 1 Train_loss 0.36584434\n",
      "Iteration 2 Train_loss 0.39030284\n",
      "Iteration 3 Train_loss 0.3914475\n",
      "Iteration 4 Train_loss 0.37924707\n",
      "Iteration 5 Train_loss 0.45544153\n",
      "Iteration 6 Train_loss 0.47557032\n",
      "Iteration 7 Train_loss 0.42400536\n",
      "Diversity: accuracy=0.51967 novelty=0.43924 aggdiv=0.05602 entropydiv=7.10277\n",
      "epochs 99\n",
      "Iteration 0 Train_loss 0.33360016\n",
      "Iteration 1 Train_loss 0.44585145\n",
      "Iteration 2 Train_loss 0.4994852\n",
      "Iteration 3 Train_loss 0.33092856\n",
      "Iteration 4 Train_loss 0.51942027\n",
      "Iteration 5 Train_loss 0.3600833\n",
      "Iteration 6 Train_loss 0.32219654\n",
      "Iteration 7 Train_loss 0.4105619\n",
      "Diversity: accuracy=0.54350 novelty=0.44298 aggdiv=0.05542 entropydiv=7.17575\n",
      "epochs 100\n",
      "Iteration 0 Train_loss 0.45976788\n",
      "Iteration 1 Train_loss 0.4792716\n",
      "Iteration 2 Train_loss 0.44552428\n",
      "Iteration 3 Train_loss 0.49633032\n",
      "Iteration 4 Train_loss 0.47862434\n",
      "Iteration 5 Train_loss 0.45622835\n",
      "Iteration 6 Train_loss 0.4044214\n",
      "Iteration 7 Train_loss 0.40902257\n",
      "Diversity: accuracy=0.54231 novelty=0.46784 aggdiv=0.05399 entropydiv=7.14537\n"
     ]
    }
   ],
   "source": [
    "resys.fit(epochs=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
